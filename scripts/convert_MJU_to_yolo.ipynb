{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b3b072aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ultralytics 8.3.146  Python-3.11.9 torch-2.6.0+cu118 CUDA:0 (GeForce GTX 1650, 4096MiB)\n",
      "Setup complete  (12 CPUs, 15.9 GB RAM, 140.7/931.5 GB disk)\n"
     ]
    }
   ],
   "source": [
    "import ultralytics\n",
    "ultralytics.checks()\n",
    "import json\n",
    "import cv2\n",
    "import os\n",
    "from sklearn.model_selection import train_test_split\n",
    "import shutil\n",
    "from ultralytics import YOLO\n",
    "from pprint import pprint\n",
    "from pathlib import Path\n",
    "import random\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41c6d1ca",
   "metadata": {},
   "source": [
    "Only run this notebook once, after downloading the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70e9b449",
   "metadata": {},
   "outputs": [],
   "source": [
    "# how to get COCO formatted data: \n",
    "# download the dataset from https://datasetninja.com/mju-waste#download and place in folder 'data/mju-waste-COCO\n",
    "# clone the instances.json files from https://github.com/realwecan/mju-waste and place in folder 'data/mju-waste-COCO/annotations'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "220d6056",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- INFO (showing first 2 entries) ---\n",
      "info is not a list, skipping.\n",
      "\n",
      "--- LICENSES (showing first 2 entries) ---\n",
      "licenses is not a list, skipping.\n",
      "\n",
      "--- CATEGORIES (showing first 2 entries) ---\n",
      "{'id': 0, 'name': 'Rubbish', 'supercategory': 'Waste'}\n",
      "\n",
      "--- ANNOTATIONS (showing first 2 entries) ---\n",
      "{'area': 11013.695949999998,\n",
      " 'bbox': [450, 255, 137, 204],\n",
      " 'category_id': 0,\n",
      " 'id': 1621,\n",
      " 'image_id': 1617,\n",
      " 'iscrowd': 0,\n",
      " 'segmentation': [318.11,\n",
      "                  188.75,\n",
      "                  332.25,\n",
      "                  324.48,\n",
      "                  414.96,\n",
      "                  315.99,\n",
      "                  395.17,\n",
      "                  180.27,\n",
      "                  318.11,\n",
      "                  188.04]}\n",
      "{'area': 13896.986949999991,\n",
      " 'bbox': [403, 228, 149, 198],\n",
      " 'category_id': 0,\n",
      " 'id': 1622,\n",
      " 'image_id': 1618,\n",
      " 'iscrowd': 0,\n",
      " 'segmentation': [287.01,\n",
      "                  164.71,\n",
      "                  385.98,\n",
      "                  162.59,\n",
      "                  390.22,\n",
      "                  296.2,\n",
      "                  285.6,\n",
      "                  301.15,\n",
      "                  284.89,\n",
      "                  161.18]}\n",
      "\n",
      "--- IMAGES (showing first 2 entries) ---\n",
      "{'coco_url': '',\n",
      " 'date_captured': '2019-11-21 16:19:37',\n",
      " 'file_name': '2019-09-19_16_19_32-29_color.png',\n",
      " 'flickr_url': '',\n",
      " 'height': 480,\n",
      " 'id': 1617,\n",
      " 'license': 1,\n",
      " 'width': 640}\n",
      "{'coco_url': '',\n",
      " 'date_captured': '2019-11-21 16:19:37',\n",
      " 'file_name': '2019-09-19_16_19_44-93_color.png',\n",
      " 'flickr_url': '',\n",
      " 'height': 480,\n",
      " 'id': 1618,\n",
      " 'license': 1,\n",
      " 'width': 640}\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def show_first_two_per_category(json_path):\n",
    "    \"\"\"\n",
    "    Prints the first two entries of each root-level list in a JSON file.\n",
    "\n",
    "    Useful for quickly inspecting the structure and content of a COCO-style\n",
    "    annotations json file.\n",
    "\n",
    "    It pretty-prints the first two entries of each top-level key that contains a list.\n",
    "\n",
    "    Args:\n",
    "        json_path (str or Path): Path to the JSON file to inspect.\n",
    "\n",
    "    Raises:\n",
    "        FileNotFoundError: If the provided path does not point to an existing file.\n",
    "        json.JSONDecodeError: If the file is not valid JSON.\n",
    "    \"\"\"\n",
    "    json_path = Path(json_path)\n",
    "\n",
    "    if not json_path.exists():\n",
    "        print(f\"File not found: {json_path}\")\n",
    "        return\n",
    "\n",
    "    with open(json_path, 'r') as f:\n",
    "        data = json.load(f)\n",
    "\n",
    "    for key, value in data.items():\n",
    "        print(f\"\\n--- {key.upper()} (showing first 2 entries) ---\")\n",
    "        if isinstance(value, list):\n",
    "            for item in value[:2]:\n",
    "                pprint(item)\n",
    "        else:\n",
    "            print(f\"{key} is not a list, skipping.\")\n",
    "\n",
    "\n",
    "show_first_two_per_category('..\\\\data\\\\mju-waste-COCO\\\\annotations\\\\train.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3c67a077",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved combined cleaned annotations to ..\\data\\mju-waste-COCO\\clean_annotations\\annotations.json\n",
      "Total images: 2475\n",
      "Total annotations: 2532\n"
     ]
    }
   ],
   "source": [
    "# This script combines the train, val, and test JSON files from the MJU Waste dataset into a single COCO-style JSON file.\n",
    "# it also cleans the annotation files, only keeping annotation id, image_id, category_id (set to 0 for 'trash'), bbox, area, and iscrowd.\n",
    "\n",
    "# Set paths\n",
    "input_dir = Path(\"..\") / \"data\" / \"mju-waste-COCO\" / \"annotations\"\n",
    "output_file = Path(\"..\") / \"data\" / \"mju-waste-COCO\" / \"clean_annotations\" / \"annotations.json\"\n",
    "output_file.parent.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Files to process\n",
    "splits = ['train', 'val', 'test']\n",
    "\n",
    "combined_images = []\n",
    "combined_annotations = []\n",
    "annotation_id = 0\n",
    "image_ids_seen = set()\n",
    "\n",
    "for split in splits:\n",
    "    input_file = input_dir / f\"{split}.json\"\n",
    "\n",
    "    with input_file.open('r') as f:\n",
    "        data = json.load(f)\n",
    "\n",
    "    for img in data.get(\"images\", []):\n",
    "        if img[\"id\"] not in image_ids_seen:\n",
    "            combined_images.append(img)\n",
    "            image_ids_seen.add(img[\"id\"])\n",
    "\n",
    "    for ann in data.get(\"annotations\", []):\n",
    "        cleaned_ann = {\n",
    "            'id': annotation_id,\n",
    "            'image_id': ann['image_id'],\n",
    "            'category_id': 0,  # unify to 'trash'\n",
    "            'bbox': ann['bbox'],\n",
    "            'area': ann['area'],\n",
    "            'iscrowd': ann.get('iscrowd', 0)\n",
    "        }\n",
    "        combined_annotations.append(cleaned_ann)\n",
    "        annotation_id += 1\n",
    "\n",
    "# Set single category at the end\n",
    "categories = [{\"id\": 0, \"name\": \"trash\"}]\n",
    "\n",
    "# Build and save final dataset\n",
    "cleaned_data = {\n",
    "    'images': combined_images,\n",
    "    'annotations': combined_annotations,\n",
    "    'categories': categories\n",
    "}\n",
    "\n",
    "with output_file.open('w') as f:\n",
    "    json.dump(cleaned_data, f)\n",
    "\n",
    "print(f\"Saved combined cleaned annotations to {output_file}\")\n",
    "print(f\"Total images: {len(combined_images)}\")\n",
    "print(f\"Total annotations: {len(combined_annotations)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e5586872",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- IMAGES (showing first 2 entries) ---\n",
      "{'coco_url': '',\n",
      " 'date_captured': '2019-11-21 16:19:37',\n",
      " 'file_name': '2019-09-19_16_19_32-29_color.png',\n",
      " 'flickr_url': '',\n",
      " 'height': 480,\n",
      " 'id': 1617,\n",
      " 'license': 1,\n",
      " 'width': 640}\n",
      "{'coco_url': '',\n",
      " 'date_captured': '2019-11-21 16:19:37',\n",
      " 'file_name': '2019-09-19_16_19_44-93_color.png',\n",
      " 'flickr_url': '',\n",
      " 'height': 480,\n",
      " 'id': 1618,\n",
      " 'license': 1,\n",
      " 'width': 640}\n",
      "\n",
      "--- ANNOTATIONS (showing first 2 entries) ---\n",
      "{'area': 11013.695949999998,\n",
      " 'bbox': [450, 255, 137, 204],\n",
      " 'category_id': 0,\n",
      " 'id': 0,\n",
      " 'image_id': 1617,\n",
      " 'iscrowd': 0}\n",
      "{'area': 13896.986949999991,\n",
      " 'bbox': [403, 228, 149, 198],\n",
      " 'category_id': 0,\n",
      " 'id': 1,\n",
      " 'image_id': 1618,\n",
      " 'iscrowd': 0}\n",
      "\n",
      "--- CATEGORIES (showing first 2 entries) ---\n",
      "{'id': 0, 'name': 'trash'}\n"
     ]
    }
   ],
   "source": [
    "show_first_two_per_category('..\\\\data\\\\mju-waste-COCO\\\\clean_annotations\\\\annotations.json')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e00a52ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This cell sets up the function that converts the simple COCO dataset to YOLO format\n",
    "\n",
    "def convert_coco_to_yolo(coco_root: Path, dataset_name: str, train_split: float = 0.8):\n",
    "    \"\"\"\n",
    "    Converts a simple COCO dataset to YOLOv8 format, including train/val split and data.yaml generation.\n",
    "\n",
    "    Args:\n",
    "        coco_root (Path): Path to the root of the simple COCO dataset (should contain images/ and annotations.json).\n",
    "        dataset_name (str): Name of the output dataset folder (e.g., \"taco\" -> creates \"taco_yolo\").\n",
    "        train_split (float, optional): Fraction of images to use for training. Defaults to 0.8.\n",
    "          The remaining images are split between validation and testing.\n",
    "\n",
    "    Returns:\n",
    "        Path: Path to the data.yaml file\n",
    "    \"\"\"\n",
    "    # Paths\n",
    "    coco_json_path = coco_root / 'annotations.json'\n",
    "    coco_images_path = coco_root / 'images'\n",
    "\n",
    "    # Load COCO JSON and get number of images for naming\n",
    "    with open(coco_json_path, 'r') as f:\n",
    "        coco = json.load(f)\n",
    "    n_total = len(coco['images'])\n",
    "\n",
    "    # Paths con't\n",
    "    yolo_root = coco_root.parent / f\"{dataset_name}_yolo_{n_total}\"\n",
    "    yolo_img_dirs = {\n",
    "        'train': yolo_root / 'images' / 'train',\n",
    "        'val': yolo_root / 'images' / 'val',\n",
    "        'test': yolo_root / 'images' / 'test',\n",
    "    }\n",
    "    yolo_lbl_dirs = {\n",
    "        'train': yolo_root / 'labels' / 'train',\n",
    "        'val': yolo_root / 'labels' / 'val',\n",
    "        'test': yolo_root / 'labels' / 'test',\n",
    "    }\n",
    "\n",
    "    # Clear and recreate folders\n",
    "    for d in list(yolo_img_dirs.values()) + list(yolo_lbl_dirs.values()):\n",
    "        if d.exists():\n",
    "            shutil.rmtree(d)\n",
    "        d.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    # Map image_id -> metadata\n",
    "    image_info = {img['id']: (img['width'], img['height'], img['file_name']) for img in coco['images']}\n",
    "\n",
    "    # Map image_id -> annotations\n",
    "    annots_per_image = defaultdict(list)\n",
    "    for ann in coco['annotations']:\n",
    "        annots_per_image[ann['image_id']].append(ann)\n",
    "\n",
    "    # Shuffle and split image IDs\n",
    "    all_image_ids = list(image_info.keys())\n",
    "    random.shuffle(all_image_ids)\n",
    "    n_total = len(all_image_ids)\n",
    "    n_train = int(n_total * train_split)\n",
    "    n_val = int((n_total - n_train) / 2)\n",
    "    n_test = n_total - n_train - n_val\n",
    "\n",
    "    split_ids = {\n",
    "        'train': set(all_image_ids[:n_train]),\n",
    "        'val': set(all_image_ids[n_train:n_train + n_val]),\n",
    "        'test': set(all_image_ids[n_train + n_val:]),\n",
    "    }\n",
    "\n",
    "    def write_labels_and_copy_images(image_ids, img_dir, lbl_dir):\n",
    "        for image_id in image_ids:\n",
    "            width, height, filename = image_info[image_id]\n",
    "            orig_stem = Path(filename).stem\n",
    "            new_stem = f\"{dataset_name}_{orig_stem}\"\n",
    "            label_path = lbl_dir / f\"{new_stem}.txt\"\n",
    "            image_src = coco_images_path / filename\n",
    "            image_dst = img_dir / f\"{new_stem}.jpg\"\n",
    "\n",
    "            # Copy image\n",
    "            if image_src.exists():\n",
    "                shutil.copy(image_src, image_dst)\n",
    "            else:\n",
    "                print(f\"Warning: Image not found: {image_src}\")\n",
    "                continue\n",
    "\n",
    "            # Write labels\n",
    "            with open(label_path, 'w') as f:\n",
    "                for ann in annots_per_image.get(image_id, []):\n",
    "                    class_id = ann['category_id']\n",
    "                    x, y, w, h = ann['bbox']\n",
    "                    x_center = (x + w / 2) / width\n",
    "                    y_center = (y + h / 2) / height\n",
    "                    w /= width\n",
    "                    h /= height\n",
    "                    f.write(f\"{class_id} {x_center:.6f} {y_center:.6f} {w:.6f} {h:.6f}\\n\")\n",
    "\n",
    "    # Process splits\n",
    "    for split in ['train', 'val', 'test']:\n",
    "        write_labels_and_copy_images(\n",
    "            split_ids[split],\n",
    "            yolo_img_dirs[split],\n",
    "            yolo_lbl_dirs[split]\n",
    "        )\n",
    "\n",
    "\n",
    "    print(f\"YOLO conversion complete: {yolo_root}\")\n",
    "    print(f\"  Train: {len(split_ids['train'])}, Val: {len(split_ids['val'])}, Test: {len(split_ids['test'])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "1e0a9b2c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "YOLO conversion complete: ..\\data\\mju_waste_yolo_2475\n",
      "  Train: 1980, Val: 247, Test: 248\n"
     ]
    }
   ],
   "source": [
    "convert_coco_to_yolo(\n",
    "    coco_root= Path(\"..\\\\data\\\\mju-COCO\"),\n",
    "    dataset_name=\"mju_waste\",\n",
    "    train_split=0.8\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "f946ef01",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "WindowsPath('../data/mju_waste_yolo_2475/images/train')"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "images_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "cfd2d884",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Saved annotated image to: ..\\results\\mju_waste_2019-09-24_16_30_00-69_color.jpg\n",
      "✅ Saved annotated image to: ..\\results\\mju_waste_2019-10-15_18_22_36-53_color.jpg\n",
      "✅ Saved annotated image to: ..\\results\\mju_waste_2020-01-07_17_31_45-52_color.jpg\n"
     ]
    }
   ],
   "source": [
    "\n",
    "dataset_dir = Path(\"..\\\\data\\mju_waste_yolo_2475\")\n",
    "images_dir = dataset_dir / \"images\" / \"train\"\n",
    "labels_dir = dataset_dir / \"labels\" / \"train\"\n",
    "output_dir = Path(\"..\\\\results\")\n",
    "class_names = [\"trash\"]\n",
    "\n",
    "# Get all image paths and sample 3\n",
    "image_paths = list(images_dir.glob(\"*.jpg\"))\n",
    "random_images = random.sample(image_paths, min(3, len(image_paths)))\n",
    "\n",
    "for image_path in random_images:\n",
    "    label_path = labels_dir / (image_path.stem + \".txt\")\n",
    "\n",
    "    # Read image\n",
    "    image = cv2.imread(str(image_path))\n",
    "    if image is None:\n",
    "        print(f\"Could not load image: {image_path}\")\n",
    "        continue\n",
    "    h, w = image.shape[:2]\n",
    "\n",
    "    # Read and draw bounding boxes\n",
    "    if label_path.exists():\n",
    "        with label_path.open(\"r\") as f:\n",
    "            for line in f:\n",
    "                parts = line.strip().split()\n",
    "                if len(parts) != 5:\n",
    "                    continue\n",
    "\n",
    "                cls, x_center, y_center, box_w, box_h = map(float, parts)\n",
    "                x_center *= w\n",
    "                y_center *= h\n",
    "                box_w *= w\n",
    "                box_h *= h\n",
    "\n",
    "                x1 = int(x_center - box_w / 2)\n",
    "                y1 = int(y_center - box_h / 2)\n",
    "                x2 = int(x_center + box_w / 2)\n",
    "                y2 = int(y_center + box_h / 2)\n",
    "\n",
    "                # Draw bounding box and class label\n",
    "                label = class_names[int(cls)] if int(cls) < len(class_names) else str(int(cls))\n",
    "                cv2.rectangle(image, (x1, y1), (x2, y2), (0, 255, 0), 2)\n",
    "                cv2.putText(image, label, (x1, y1 - 8), cv2.FONT_HERSHEY_SIMPLEX,\n",
    "                            0.6, (0, 255, 0), 2)\n",
    "    else:\n",
    "        print(f\"No label found for {image_path.name}\")\n",
    "\n",
    "    # Show image\n",
    "    output_path = output_dir / image_path.name\n",
    "    cv2.imwrite(str(output_path), image)\n",
    "    print(f\"✅ Saved annotated image to: {output_path}\")\n",
    "\n",
    "cv2.destroyAllWindows()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "CUDA_py3.11",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
