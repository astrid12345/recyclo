{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4JBnzl6uHvDx"
      },
      "outputs": [],
      "source": [
        "# Reading 3 zipped datasets from Google Drive and unzipping them\n",
        "\n",
        "import zipfile\n",
        "from pathlib import Path\n",
        "import gdown\n",
        "import yaml\n",
        "import shutil # Import shutil for file copying\n",
        "\n",
        "# Needs to be updated with the most up-to-date datasets\n",
        "\n",
        "datasets = {\n",
        "    '1hb-7KYjd_H-KPprpB7Pv4-fO2aKvET6m':'AquaTrash_yolo',\n",
        "    '1Z9xElXVKoj62XCrYz2ZmRpzBh8Rgy2ue':'mju_waste_yolo',\n",
        "    '1-2imLxXKszSvYGXkmrz3kbDfxHh2C5VR':'TACO_yolo'\n",
        "}\n",
        "\n",
        "# === Root paths ===\n",
        "\n",
        "project_root = Path('/content')\n",
        "zip_dir = project_root/'zips'\n",
        "zip_dir.mkdir(parents=True, exist_ok=True)\n",
        "downloaded_zip_files = []\n",
        "extracted_yolo_dataset_paths = []\n",
        "\n",
        "for file_id, friendly_name in datasets.items():\n",
        "    zip_path = zip_dir/f\"{friendly_name}.zip\"\n",
        "    extract_dir = project_root/friendly_name\n",
        "    yaml_path = extract_dir/'data.yaml'\n",
        "\n",
        "    # === Download ===\n",
        "\n",
        "    url = f\"https://drive.google.com/uc?id={file_id}\"\n",
        "    print(f\"Downloading {friendly_name} from {url} ...\")\n",
        "    gdown.download(url, str(zip_path), quiet=False)\n",
        "    downloaded_zip_files.append(zip_path.name)\n",
        "\n",
        "    # === Extract ===\n",
        "\n",
        "    print(f\"Extracting to {extract_dir} ...\")\n",
        "    # Ensure the target extraction directory exists\n",
        "    extract_dir.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "    with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
        "        members = zip_ref.namelist()\n",
        "\n",
        "        top_level_dir = None\n",
        "        if members:\n",
        "            first_member = members[0]\n",
        "            if '/' in first_member and first_member.endswith('/'):\n",
        "                top_level_dir = first_member.split('/')[0] + '/'\n",
        "\n",
        "        for member in members:\n",
        "            if top_level_dir and member.startswith(top_level_dir):\n",
        "                extracted_path = member[len(top_level_dir):]\n",
        "            else:\n",
        "                extracted_path = member\n",
        "\n",
        "            if extracted_path:\n",
        "                target_path = extract_dir/extracted_path\n",
        "                target_path.parent.mkdir(parents=True, exist_ok=True)\n",
        "                if not member.endswith('/'):\n",
        "                    with zip_ref.open(member) as source, open(target_path, \"wb\") as target:\n",
        "                        shutil.copyfileobj(source, target)\n",
        "\n",
        "    extracted_yolo_dataset_paths.append(str(extract_dir))\n",
        "\n",
        "    # === Update data.yaml ===\n",
        "\n",
        "    if yaml_path.exists():\n",
        "        print(f\"Updating path in {yaml_path} ...\")\n",
        "        with yaml_path.open('r') as f:\n",
        "            data = yaml.safe_load(f)\n",
        "        data['path'] = str(extract_dir)\n",
        "        with yaml_path.open('w') as f:\n",
        "            yaml.dump(data, f)\n",
        "        print(f\"Updated 'path' in data.yaml to: {extract_dir}\\n\")\n",
        "    else:\n",
        "        print(f\"Warning: data.yaml not found in {extract_dir}\\n\")\n",
        "\n",
        "# === Summary ===\n",
        "\n",
        "print(\"\\nDownloaded ZIP files:\")\n",
        "for name in downloaded_zip_files:\n",
        "    print(f\"- {name}\")\n",
        "print(\"\\nExtracted dataset folders:\")\n",
        "for path in extracted_yolo_dataset_paths:\n",
        "    print(f\"- {path}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zCuriRV9IlFw"
      },
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "-Shuffles all image and labels from all 3 datasets and does another global train, val, test split once all 3 are merged.\n",
        "-Keeps dataset name prefixes in all image and label names as in their original filenames.\n",
        "-Creates a .yaml file for the merged dataset.\n",
        "- Skips images with no label files.\n",
        "\n",
        "\"\"\"\n",
        "import os\n",
        "import shutil\n",
        "import yaml\n",
        "import random\n",
        "\n",
        "# --- Configuration ---\n",
        "\n",
        "# Output directory for the merged YOLO dataset\n",
        "merged_output_dir = '/content/merged_yolo_dataset_reshuffled' # Using a new directory name to avoid conflicts\n",
        "\n",
        "# Define split ratios\n",
        "# Ensure these variables are defined before being used\n",
        "train_split_ratio = 0.8\n",
        "val_split_ratio = 0.1\n",
        "test_split_ratio = 0.1 # The remaining percentage\n",
        "\n",
        "# Create the main merged directories\n",
        "merged_images_base = os.path.join(merged_output_dir, 'images')\n",
        "merged_labels_base = os.path.join(merged_output_dir, 'labels')\n",
        "\n",
        "# Create train, val, test subdirectories within the merged structure\n",
        "for subdir in ['train', 'val', 'test']:\n",
        "    os.makedirs(os.path.join(merged_images_base, subdir), exist_ok=True)\n",
        "    os.makedirs(os.path.join(merged_labels_base, subdir), exist_ok=True)\n",
        "\n",
        "# Keep track of encountered class names and map them to new indices\n",
        "# Since we are mapping to a single 'trash' class, these are less critical for the final output labels,\n",
        "# but are kept for information during collection.\n",
        "class_names_set = set()\n",
        "merged_class_names_info = [] # To store original class names encountered\n",
        "class_name_to_id = {} # This will only contain {'trash': 0} in the end\n",
        "\n",
        "# List to store all image-label file pairs\n",
        "all_files = [] # Initialize all_files here\n",
        "\n",
        "print(\"Collecting all image and label file paths...\")\n",
        "\n",
        "# Make sure 'extracted_yolo_dataset_paths' is defined from a previous cell or execution.\n",
        "# If not, you'll get a NameError for 'extracted_yolo_dataset_paths'.\n",
        "# Assuming it is defined from the cell above (ipython-input-10).\n",
        "\n",
        "for dataset_path in extracted_yolo_dataset_paths:\n",
        "    print(f\"Processing dataset: {dataset_path}\")\n",
        "\n",
        "    # Extract a simple prefix from the dataset path (kept for potential debugging/info, not used for renaming)\n",
        "    dataset_prefix = os.path.basename(dataset_path).replace('_extracted', '').replace('-', '_') + '_'\n",
        "\n",
        "    # --- Process Class Names from data.yaml if available ---\n",
        "    data_yaml_path = os.path.join(dataset_path, 'data.yaml')\n",
        "    current_class_id_to_name = {} # Store the original class map for this dataset\n",
        "\n",
        "    if os.path.exists(data_yaml_path):\n",
        "        try:\n",
        "            with open(data_yaml_path, 'r') as f:\n",
        "                current_data_yaml = yaml.safe_load(f)\n",
        "                current_class_names = current_data_yaml.get('names', [])\n",
        "                current_class_id_to_name = {i: name for i, name in enumerate(current_class_names)}\n",
        "\n",
        "            # Collect original class names encountered\n",
        "            for original_class_name in current_class_names:\n",
        "                if original_class_name not in class_names_set:\n",
        "                    class_names_set.add(original_class_name)\n",
        "                    merged_class_names_info.append(original_class_name) # Store original names\n",
        "            print(f\"  Found {len(current_class_names)} original classes in data.yaml.\")\n",
        "\n",
        "        except FileNotFoundError:\n",
        "             print(f\"  Warning: data.yaml not found at {data_yaml_path}. Cannot get class names from this dataset.\")\n",
        "        except Exception as e:\n",
        "             print(f\"  Error loading data.yaml from {data_yaml_path}: {e}. Skipping class processing for this dataset.\")\n",
        "    else:\n",
        "         print(f\"  Warning: data.yaml not found at {data_yaml_path}. Cannot get class names from this dataset.\")\n",
        "\n",
        "\n",
        "    # --- Collect all image and label pairs from all available splits ---\n",
        "    for split_subdir in ['train', 'val', 'test']: # Iterate through potential splits\n",
        "        original_images_path = os.path.join(dataset_path, 'images', split_subdir)\n",
        "        original_labels_path = os.path.join(dataset_path, 'labels', split_subdir)\n",
        "\n",
        "        if os.path.exists(original_images_path) and os.path.exists(original_labels_path):\n",
        "            print(f\"  Collecting files from {split_subdir} split...\")\n",
        "            for filename in os.listdir(original_images_path):\n",
        "                img_name, img_ext = os.path.splitext(filename)\n",
        "                label_filename = img_name + '.txt'\n",
        "                original_image_file = os.path.join(original_images_path, filename)\n",
        "                original_label_file = os.path.join(original_labels_path, label_filename)\n",
        "\n",
        "                # Check if corresponding label file exists\n",
        "                if os.path.exists(original_label_file):\n",
        "                    # Store the original paths and the dataset prefix for renaming later\n",
        "                    all_files.append({\n",
        "                        'original_img': original_image_file,\n",
        "                        'original_label': original_label_file,\n",
        "                        'dataset_prefix': dataset_prefix,\n",
        "                        'original_class_id_to_name': current_class_id_to_name # Store the class map for this dataset\n",
        "                    })\n",
        "                else:\n",
        "                    print(f\"    Warning: Label file not found for image {filename} at {original_labels_path}. Skipping this image and its missing label.\")\n",
        "        else:\n",
        "             print(f\"  Info: Skipping {split_subdir} for this dataset as directory does not exist: {original_images_path}\")\n",
        "\n",
        "\n",
        "print(f\"\\nTotal image-label pairs collected: {len(all_files)}\")\n",
        "\n",
        "# --- Apply Global Train/Val/Test Split ---\n",
        "print(\"Applying global train/val/test split...\")\n",
        "\n",
        "random.shuffle(all_files) # Shuffle the collected files NOW that it's populated\n",
        "\n",
        "n_total = len(all_files)\n",
        "\n",
        "# Calculate the number of files for each split based on the desired ratios\n",
        "# Make sure train_split_ratio and val_split_ratio are defined\n",
        "n_train = int(n_total * train_split_ratio)\n",
        "n_val = int(n_total * val_split_ratio) # Use the ratio for validation\n",
        "n_test = n_total - n_train - n_val # The rest goes to test\n",
        "\n",
        "train_files = all_files[:n_train]\n",
        "val_files = all_files[n_train:n_train + n_val]\n",
        "test_files = all_files[n_train + n_val:]\n",
        "\n",
        "print(f\"Split distribution: Train={len(train_files)}, Val={len(val_files)}, Test={len(test_files)}\")\n",
        "\n",
        "# --- Copy and Process Files to Merged Directory ---\n",
        "print(\"Copying and processing files to merged dataset...\")\n",
        "\n",
        "file_splits = {'train': train_files, 'val': val_files, 'test': test_files}\n",
        "\n",
        "for split_name, files_list in file_splits.items():\n",
        "    print(f\"Processing {split_name} split...\")\n",
        "    merged_images_split_path = os.path.join(merged_images_base, split_name)\n",
        "    merged_labels_split_path = os.path.join(merged_labels_base, split_name)\n",
        "\n",
        "    for file_info in files_list:\n",
        "        original_img_path = file_info['original_img']\n",
        "        original_label_path = file_info['original_label']\n",
        "        # dataset_prefix and original_class_id_to_name are available but not strictly needed for label processing with single class\n",
        "\n",
        "        # Use original filenames for copying\n",
        "        img_filename = os.path.basename(original_img_path)\n",
        "        label_filename = os.path.basename(original_label_path)\n",
        "\n",
        "        merged_dest_image_path = os.path.join(merged_images_split_path, img_filename)\n",
        "        merged_dest_label_path = os.path.join(merged_labels_split_path, label_filename)\n",
        "\n",
        "        try:\n",
        "            # Copy files\n",
        "            shutil.copy2(original_img_path, merged_dest_image_path)\n",
        "            shutil.copy2(original_label_path, merged_dest_label_path)\n",
        "\n",
        "            # --- Update Class Indices in Label File ---\n",
        "            # This section is modified to map all original classes to the single 'trash' class (ID 0)\n",
        "            updated_lines = []\n",
        "            # Use merged_dest_label_path here to open the newly copied label file\n",
        "            with open(merged_dest_label_path, 'r') as f:\n",
        "                for line in f:\n",
        "                    parts = line.strip().split()\n",
        "                    if parts and len(parts) == 5:\n",
        "                        try:\n",
        "                            # We don't need the original_class_id value itself, just the format\n",
        "                            # original_class_id = int(parts[0]) # This line is no longer strictly needed\n",
        "\n",
        "                            # Since the merged dataset has only one class 'trash' with ID 0,\n",
        "                            # map all original class IDs to 0.\n",
        "                            new_class_id = 0\n",
        "\n",
        "                            parts[0] = str(new_class_id)\n",
        "                            updated_lines.append(\" \".join(parts))\n",
        "\n",
        "                        except ValueError:\n",
        "                            # Use merged_dest_label_path here for informative warning\n",
        "                            print(f\"    Warning: Invalid class ID format in label file {os.path.basename(merged_dest_label_path)} line: '{line.strip()}'. Skipping this line.\")\n",
        "                            pass # Skip lines with invalid format\n",
        "                        except Exception as line_e:\n",
        "                            # Use merged_dest_label_path here for informative warning\n",
        "                            print(f\"    Error processing line '{line.strip()}' in {os.path.basename(merged_dest_label_path)}: {line_e}. Skipping this line.\")\n",
        "                            pass # Skip lines with other errors\n",
        "                    else:\n",
        "                        # Use merged_dest_label_path here for informative warning\n",
        "                        print(f\"    Warning: Skipping malformed line in {os.path.basename(merged_dest_label_path)}: '{line.strip()}'\")\n",
        "\n",
        "\n",
        "            # Write the updated lines back to the label file\n",
        "            # Use merged_dest_label_path here to write to the newly copied label file\n",
        "            with open(merged_dest_label_path, 'w') as f:\n",
        "                f.write(\"\\n\".join(updated_lines))\n",
        "\n",
        "        except FileNotFoundError:\n",
        "             print(f\"    Error copying files for {img_filename}. Image or label file not found unexpectedly.\")\n",
        "        except Exception as e:\n",
        "             print(f\"    Error processing file {img_filename}: {e}\") # Keep img_filename here as it's the identifier\n",
        "\n",
        "\n",
        "print(\"YOLO dataset merging complete.\")\n",
        "\n",
        "# --- Create Merged data.yaml ---\n",
        "print(\"Creating merged data.yaml...\")\n",
        "\n",
        "# Ensure the class name list contains only 'trash'\n",
        "final_merged_class_names = ['trash']\n",
        "# Update the class_name_to_id dictionary to reflect the final state\n",
        "class_name_to_id = {'trash': 0}\n",
        "\n",
        "\n",
        "merged_data_yaml = {\n",
        "    'path': merged_output_dir,\n",
        "    'train': os.path.join(merged_images_base, 'train'),\n",
        "    'val': os.path.join(merged_images_base, 'val'),\n",
        "    'test': os.path.join(merged_images_base, 'test'),\n",
        "    'nc': 1,\n",
        "    'names': ['trash']\n",
        "}\n",
        "\n",
        "# Save the merged data.yaml file\n",
        "merged_data_yaml_path = os.path.join(merged_output_dir, 'data.yaml')\n",
        "with open(merged_data_yaml_path, 'w') as f:\n",
        "    yaml.dump(merged_data_yaml, f, sort_keys=False)\n",
        "\n",
        "print(f\"Merged data.yaml saved to: {merged_data_yaml_path}\")\n",
        "print(f\"Total classes in merged dataset: {len(final_merged_class_names)}\")\n",
        "print(f\"Merged class names: {final_merged_class_names}\")\n",
        "\n",
        "# --- Next Steps ---\n",
        "print(\"\\nNext Steps:\")\n",
        "print(f\"1. Your reshuffled merged YOLO dataset is located at: {merged_output_dir}\")\n",
        "print(f\"2. The data.yaml file is in: {merged_data_yaml_path}\")\n",
        "print(\"3. You can now use this merged dataset for training with YOLO.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pwWO735tpKEB"
      },
      "outputs": [],
      "source": [
        "# Summarizing the % of images from all 3 datasets in train, val, test datasets\n",
        "# %%\n",
        "import os\n",
        "\n",
        "# Path to your merged dataset directory\n",
        "merged_output_dir = '/content/merged_yolo_dataset_reshuffled' # Ensure this matches your output directory name if you changed it in the merging step\n",
        "\n",
        "# Define characteristic filename patterns for each original dataset\n",
        "# We'll use these to identify which dataset an image originally came from\n",
        "dataset_patterns = {\n",
        "    'mju_waste': 'mju_waste_', # mju-waste images seem to start with '20' (e.g., '2019-...')\n",
        "    'TACO': 'TACO_',    # TACO images start with 'TACO_'\n",
        "    'AquaTrash': 'AquaTrash_' # AquaTrash images start with 'AquaTrash_'\n",
        "}\n",
        "\n",
        "# Dictionary to store counts\n",
        "# Structure: {split: {pattern_key: count}}\n",
        "image_counts_by_dataset = {}\n",
        "\n",
        "# Dictionary to store total counts per split\n",
        "total_images_per_split = {}\n",
        "\n",
        "print(\"\\nCounting images from each original dataset in train, val, and test splits...\")\n",
        "\n",
        "total_image_label_pairs_collected = 0 # Initialize a counter for total images across all splits\n",
        "\n",
        "# Iterate through each split (train, val, test)\n",
        "for split_subdir in ['train', 'val', 'test']:\n",
        "    split_images_path = os.path.join(merged_output_dir, 'images', split_subdir)\n",
        "    image_counts_by_dataset[split_subdir] = {key: 0 for key in dataset_patterns.keys()} # Initialize counts for this split\n",
        "    total_images_per_split[split_subdir] = 0 # Initialize total for this split\n",
        "\n",
        "    if os.path.exists(split_images_path):\n",
        "        print(f\"Processing split: {split_subdir}\")\n",
        "        # List all files in the image directory for the current split\n",
        "        for filename in os.listdir(split_images_path):\n",
        "            # Check if the file is an image (you might want to add more robust checks for image extensions)\n",
        "            if filename.lower().endswith(('.png', '.jpg', '.jpeg', '.bmp', '.tiff')):\n",
        "                # Try to match the filename with a known dataset pattern\n",
        "                matched_key = None\n",
        "                for key, pattern in dataset_patterns.items():\n",
        "                    if filename.startswith(pattern):\n",
        "                        matched_key = key\n",
        "                        break # Found a match, no need to check other patterns\n",
        "\n",
        "                if matched_key:\n",
        "                    # Increment the count for the matched pattern in the current split\n",
        "                    image_counts_by_dataset[split_subdir][matched_key] += 1\n",
        "                    total_images_per_split[split_subdir] += 1 # Increment total for this split\n",
        "                    total_image_label_pairs_collected += 1 # Increment overall total\n",
        "                else:\n",
        "                    # Handle files that don't match any expected pattern\n",
        "                    print(f\"  Warning: Image '{filename}' in {split_subdir} does not match any known dataset pattern. It will not be included in the per-dataset counts.\")\n",
        "    else:\n",
        "        print(f\"  Info: Image directory for {split_subdir} not found at {split_images_path}\")\n",
        "\n",
        "# Print the total number of image-label pairs collected across all splits\n",
        "print(f\"\\nTotal image-label pairs collected across all merged splits: {total_image_label_pairs_collected}\")\n",
        "\n",
        "# Print the results per dataset per split\n",
        "print(\"\\n--- Image Counts Summary (per original dataset within each split) ---\")\n",
        "for split_name, pattern_counts in image_counts_by_dataset.items():\n",
        "    print(f\"\\nSplit: {split_name}\")\n",
        "    total_in_split = total_images_per_split[split_name] # Use the total calculated during the file scan\n",
        "    if total_in_split > 0:\n",
        "        for key, count in pattern_counts.items():\n",
        "            # Use the key for display name\n",
        "            display_name = key\n",
        "            print(f\"  - {display_name}: {count} images ({count/total_in_split:.1%})\")\n",
        "        print(f\"  Total images in {split_name} split: {total_in_split}\")\n",
        "    else:\n",
        "        print(f\"  No images found in the {split_name} split.\")\n",
        "\n",
        "# Print the overall train/val/test split percentages\n",
        "print(\"\\n--- Overall Train/Val/Test Split Summary ---\")\n",
        "\n",
        "if total_image_label_pairs_collected > 0:\n",
        "    for split_name, count in total_images_per_split.items():\n",
        "        percentage = (count / total_image_label_pairs_collected) * 100 if total_image_label_pairs_collected > 0 else 0\n",
        "        print(f\"  - {split_name}: {count} images ({percentage:.1f}%)\")\n",
        "else:\n",
        "    print(\"No images found in the merged dataset to calculate split percentages.\")\n",
        "\n",
        "print(\"\\n--- End of Summary ---\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jY9jbtiUbWQP"
      },
      "outputs": [],
      "source": [
        "\"\"\"# saving the merged_yolo-dataset_reshuffle as a zip file to be downloaded\n",
        "\n",
        "import shutil\n",
        "import os\n",
        "from google.colab import files\n",
        "\n",
        "# Specify the directory you want to zip\n",
        "directory_to_zip = '/content/merged_yolo_dataset_reshuffled' # Replace with the actual path to your directory\n",
        "\n",
        "# Specify the name for the output zip file\n",
        "output_zip_file = 'merged_yolo_dataset_reshuffled.zip' # Replace with your desired zip file name\n",
        "\n",
        "# Use shutil.make_archive to create the zip file\n",
        "# The first argument is the base name of the archive (without extension)\n",
        "# The second argument is the archive format ('zip')\n",
        "# The third argument is the directory to archive\n",
        "shutil.make_archive(output_zip_file.replace('.zip', ''), 'zip', directory_to_zip)\n",
        "\n",
        "print(f\"Created zip file: {output_zip_file}\")\n",
        "\n",
        "# Download the zip file\n",
        "try:\n",
        "    files.download(output_zip_file)\n",
        "    print(f\"Downloading {output_zip_file}...\")\n",
        "except FileNotFoundError:\n",
        "    print(f\"Error: The file {output_zip_file} was not found after zipping.\")\n",
        "except Exception as e:\n",
        "    print(f\"An error occurred during download: {e}\")\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xIWG9hmzIshj"
      },
      "outputs": [],
      "source": [
        "#Inspecting a batch of x image-label pairs at a time\n",
        "\n",
        "# %%\n",
        "import os\n",
        "import random\n",
        "import yaml\n",
        "from PIL import Image, ImageDraw, ImageFont\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "# --- Configuration ---\n",
        "# Path to your merged dataset directory\n",
        "merged_output_dir = '/content/merged_yolo_dataset_reshuffled' # Ensure this matches your output directory\n",
        "\n",
        "# List of splits to check\n",
        "splits_to_check = ['train', 'val', 'test']\n",
        "\n",
        "# --- Visualization Limit ---\n",
        "images_per_split_to_display = 5 # Set the maximum number of images to display per split initially\n",
        "start_index = 0 # Set the starting index for the batch (0 for the first 50, 50 for the next, etc.)\n",
        "\n",
        "# --- Load Class Names from merged data.yaml ---\n",
        "merged_data_yaml_path = os.path.join(merged_output_dir, 'data.yaml')\n",
        "merged_class_names = []\n",
        "\n",
        "if os.path.exists(merged_data_yaml_path):\n",
        "    try:\n",
        "        with open(merged_data_yaml_path, 'r') as f:\n",
        "            merged_data_yaml = yaml.safe_load(f)\n",
        "            merged_class_names = merged_data_yaml.get('names', [])\n",
        "        print(f\"Loaded {len(merged_class_names)} class names from data.yaml: {merged_class_names}\")\n",
        "    except Exception as e:\n",
        "        print(f\"Error loading merged data.yaml from {merged_data_yaml_path}: {e}\")\n",
        "        merged_class_names = [] # Fallback to empty list if loading fails\n",
        "else:\n",
        "    print(f\"Error: Merged data.yaml not found at {merged_data_yaml_path}. Cannot display class names.\")\n",
        "    merged_class_names = []\n",
        "\n",
        "# --- Function to Draw Bounding Boxes and Display ---\n",
        "def plot_yolo_boxes(image_path, label_path, class_names):\n",
        "    try:\n",
        "        image = Image.open(image_path).convert(\"RGB\")\n",
        "        draw = ImageDraw.Draw(image)\n",
        "        width, height = image.size\n",
        "\n",
        "        with open(label_path, 'r') as f:\n",
        "            labels = f.readlines()\n",
        "\n",
        "        # If no labels, just show the image and print a note\n",
        "        if not labels or all(line.strip() == '' for line in labels):\n",
        "            print(f\"  Info: No annotations found in {os.path.basename(label_path)}. Displaying image only.\")\n",
        "            plt.figure(figsize=(10, 10))\n",
        "            plt.imshow(image)\n",
        "            plt.title(f\"Image: {os.path.basename(image_path)}\")\n",
        "            plt.axis('off')\n",
        "            plt.show()\n",
        "            return # Exit function if no labels\n",
        "\n",
        "        for label in labels:\n",
        "            parts = label.strip().split()\n",
        "            if len(parts) == 5:\n",
        "                try:\n",
        "                    class_id = int(parts[0])\n",
        "                    # YOLO format: class_id center_x center_y width height (normalized)\n",
        "                    center_x, center_y, box_width, box_height = map(float, parts[1:])\n",
        "\n",
        "                    # Convert normalized coordinates to pixel coordinates\n",
        "                    # top_left_x, top_left_y, bottom_right_x, bottom_right_y\n",
        "                    x_min = int((center_x - box_width / 2) * width)\n",
        "                    y_min = int((center_y - box_height / 2) * height)\n",
        "                    x_max = int((center_x + box_width / 2) * width)\n",
        "                    y_max = int((center_y + box_height / 2) * height)\n",
        "\n",
        "                    # Ensure coordinates are within image bounds\n",
        "                    x_min = max(0, x_min)\n",
        "                    y_min = max(0, y_min)\n",
        "                    x_max = min(width, x_max)\n",
        "                    y_max = min(height, y_max)\n",
        "\n",
        "                    # Get class name\n",
        "                    class_name = class_names[class_id] if class_id < len(class_names) else f'Unknown Class ID: {class_id}'\n",
        "\n",
        "                    # Draw rectangle\n",
        "                    draw.rectangle([(x_min, y_min), (x_max, y_max)], outline=\"red\", width=2)\n",
        "\n",
        "                    # Draw class name text\n",
        "                    try:\n",
        "                        # Use a default font or try to load one\n",
        "                        font_size = max(10, int(0.02 * height)) # Adjust font size based on image height\n",
        "                        # Attempt to use a common font or load default\n",
        "                        try:\n",
        "                            font = ImageFont.truetype(\"arial.ttf\", font_size)\n",
        "                        except IOError:\n",
        "                            try:\n",
        "                                font = ImageFont.truetype(\"DejaVuSans.ttf\", font_size)\n",
        "                            except IOError:\n",
        "                                font = ImageFont.load_default() # Fallback font\n",
        "                    except Exception as font_e:\n",
        "                         print(f\"  Warning: Could not load specific font for {os.path.basename(image_path)}: {font_e}. Using default.\")\n",
        "                         font = ImageFont.load_default()\n",
        "\n",
        "\n",
        "                    text = f\"{class_name}\"\n",
        "                    # Calculate text bounding box for positioning\n",
        "                    try:\n",
        "                        text_bbox = draw.textbbox((0, 0), text, font=font)\n",
        "                        text_width = text_bbox[2] - text_bbox[0]\n",
        "                        text_height = text_bbox[3] - text_bbox[1]\n",
        "                    except NotImplementedError:\n",
        "                        # Fallback for older Pillow versions or complex text layouts\n",
        "                        text_size = draw.textsize(text, font=font)\n",
        "                        text_width, text_height = text_size\n",
        "                        print(f\"  Warning: Using fallback text size calculation for {os.path.basename(image_path)}. Text positioning might be less precise.\")\n",
        "\n",
        "\n",
        "                    # Position the text just above the bounding box\n",
        "                    text_x = x_min\n",
        "                    text_y = y_min - text_height - 2 # 2 pixels padding\n",
        "\n",
        "                    # Ensure text is within image bounds\n",
        "                    text_x = max(0, text_x)\n",
        "                    text_y = max(0, text_y)\n",
        "                    if text_x + text_width > width:\n",
        "                         text_x = width - text_width # Move text left if it goes off right edge\n",
        "                    if text_y < 0:\n",
        "                         text_y = y_min + 2 # Place below if it goes above the image\n",
        "\n",
        "\n",
        "                    draw.text((text_x, text_y), text, fill=\"red\", font=font)\n",
        "\n",
        "                except (ValueError, IndexError) as e:\n",
        "                    print(f\"  Warning: Skipping malformed or invalid line in label file {os.path.basename(label_path)}: '{label.strip()}' - Error: {e}\")\n",
        "                    continue # Skip this annotation if parsing fails\n",
        "            else:\n",
        "                 print(f\"  Warning: Skipping malformed line in label file {os.path.basename(label_path)}: '{label.strip()}'\")\n",
        "\n",
        "\n",
        "        # Display the image using matplotlib\n",
        "        plt.figure(figsize=(10, 10))\n",
        "        plt.imshow(image)\n",
        "        plt.title(f\"Image: {os.path.basename(image_path)}\")\n",
        "        plt.axis('off')\n",
        "        plt.show()\n",
        "\n",
        "    except FileNotFoundError:\n",
        "        print(f\"  Error: Image or label file not found for {os.path.basename(image_path)}\")\n",
        "    except Exception as e:\n",
        "        print(f\"  An error occurred while processing {os.path.basename(image_path)}: {e}\")\n",
        "\n",
        "\n",
        "# --- Collect and Display a Batch of Samples per Split ---\n",
        "print(\"\\nCollecting and displaying a batch of image-label pairs from each split...\")\n",
        "\n",
        "for split_to_sample in splits_to_check:\n",
        "    image_dir = os.path.join(merged_output_dir, 'images', split_to_sample)\n",
        "    label_dir = os.path.join(merged_output_dir, 'labels', split_to_sample)\n",
        "\n",
        "    image_label_pairs = []\n",
        "\n",
        "    if os.path.exists(image_dir) and os.path.exists(label_dir):\n",
        "        print(f\"\\nCollecting image-label pairs from {split_to_sample} split...\")\n",
        "        # Collect ALL image-label pairs for this split first\n",
        "        for filename in os.listdir(image_dir):\n",
        "            img_name, img_ext = os.path.splitext(filename)\n",
        "            label_filename = img_name + '.txt'\n",
        "            image_path = os.path.join(image_dir, filename)\n",
        "            label_path = os.path.join(label_dir, label_filename)\n",
        "\n",
        "            # Check if corresponding label file exists\n",
        "            if os.path.exists(label_path):\n",
        "                image_label_pairs.append({'image': image_path, 'label': label_path})\n",
        "            # Optional: Uncomment if you want to see warnings for images with missing labels\n",
        "            # else:\n",
        "            #      print(f\"  Warning: Label file not found for image {filename} in {split_to_sample}.\")\n",
        "\n",
        "        print(f\"Found {len(image_label_pairs)} image-label pairs in {split_to_sample} split.\")\n",
        "\n",
        "        # --- Select and Display a Batch ---\n",
        "        end_index = start_index + images_per_split_to_display\n",
        "        batch_to_display = image_label_pairs[start_index:end_index]\n",
        "\n",
        "        if len(batch_to_display) > 0:\n",
        "            print(f\"\\nDisplaying batch of {len(batch_to_display)} samples (index {start_index} to {end_index-1}) from the {split_to_sample} split.\")\n",
        "            for sample in batch_to_display:\n",
        "                plot_yolo_boxes(sample['image'], sample['label'], merged_class_names)\n",
        "        else:\n",
        "            print(f\"No image-label pairs found in the {split_to_sample} split within the specified range (index {start_index} to {end_index-1}).\")\n",
        "\n",
        "    else:\n",
        "        print(f\"Error: Image or label directory not found for {split_to_sample} split.\")\n",
        "        print(f\"Image dir: {image_dir}\")\n",
        "        print(f\"Label dir: {label_dir}\")\n",
        "\n",
        "print(\"\\nFinished visualization for the current batch.\")\n",
        "\n",
        "# --- How to View the Next Batch ---\n",
        "print(f\"\\nTo view the next batch of {images_per_split_to_display} images:\")\n",
        "print(f\"1. Change the `start_index` variable in the code cell to {start_index + images_per_split_to_display}.\")\n",
        "print(\"2. Re-run the code cell.\")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
