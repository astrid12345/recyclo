{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyM7QJbw1fAdk24BFrs7gINt",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/astrid12345/recyclo/blob/convert_taco_to_yolo/scripts/convert_taco_dataset_to_yolo_format.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The purpose of this notebook is to convert a COCO-ish dataset to a format that can be used to train an Ultralytics YOLO model.\n",
        "\n",
        "# Using this notebook: workflow\n",
        "\n",
        "To use, make a copy of this notebook, and adapt it to work with your specific dataset. Please save your version of this ipynb file on GitHub in *recyclo/scripts*.\n",
        "\n",
        "(File > Save a copy in GitHub > File path = \"scripts/my_filename.ipynb\" to save notebook in scripts folder)\n",
        "\n",
        "Once you've generated your YOLO dataset, and are confident you can train a model with it, please upload your converted dataset to the Recyclo datasets google drive, https://drive.google.com/drive/folders/1bUkIYQRXX08OKI5TuOSg-eqntSudGaFB.\n",
        "\n",
        "(Why Google Drive? Because these datasets are too large for GitHub!)\n",
        "\n",
        "# What's in this notebook: contents\n",
        "\n",
        "Notebook contents:\n",
        "- intro to YOLO\n",
        "- intro to COCO\n",
        "- dataset specific notes (update for your specific dataset)\n",
        "- convert dataset to generic COCO (update for your specific dataset)\n",
        "- convert generic COCO to YOLO\n",
        "\n",
        "# Pro tips about Colabs\n",
        "\n",
        "When you open the \"Files\" tab on the left, you'll find yourself in a folder containing\n",
        "* ..\n",
        "* sample data\n",
        "\n",
        "This is a colab thing, the \"content\" folder, to get you started.\n",
        "Ignore it: click the .. to go up a level."
      ],
      "metadata": {
        "id": "WYhAfcih9-9h"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Intro to YOLO\n",
        "\n",
        "## General\n",
        "\n",
        "In general, YOLO models output the following for a given image:\n",
        "* Bounding box\n",
        "* Class label\n",
        "* Confidence score\n",
        "\n",
        "To train a YOLO model, we need object detection datasets that contain images of what we're looking for (trash), and annotations: class labels and bounding boxes.\n",
        "\n",
        "## Ultralytics YOLO\n",
        "\n",
        "In this project we will use Ultralytics YOLO object detection, eg their YOLO11n model. YOLO11n is a pretrained object detection model developed by Ultralytics.\n",
        "\n",
        "YOLO expects datasets in the following format:\n",
        "\n",
        "```\n",
        "dataset/\n",
        "├── images/\n",
        "│   ├── train/  <-- image files for training.\n",
        "│   ├── val/    <-- image files for validation after each epoch. Must not overlap with images in train.\n",
        "|   └── test/   <-- optional: can put some image files here for benchmarking.\n",
        "├── labels/\n",
        "│   ├── train/  <-- one .txt file per train image (must have same name). Contains class and bbox info..\n",
        "│   ├── val/    <-- one .txt file per val image.\n",
        "|   └── test/   <-- one .txt file per test image.\n",
        "└── data.yaml   <-- config file; helps tie all the above together.\n",
        "```\n",
        "\n",
        "Example labels/train file:\n",
        "```\n",
        "<class_id> <x_center> <y_center> <width> <height>\n",
        "```\n",
        "\n",
        "Example data.yaml file:\n",
        "```\n",
        "path: /content/dataset  # Root folder\n",
        "train: images/train\n",
        "val: images/val\n",
        "\n",
        "nc: 5  # number of classes\n",
        "names: ['bottle', 'can', 'plastic bag', 'wrapper', 'paper']\n",
        "\n",
        "```"
      ],
      "metadata": {
        "id": "x347PAo8rkYn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Intro to COCO\n",
        "\n",
        "## General\n",
        "COCO, Common Objects in Context, is a object detection, segmentation, and captioning dataset developed by Microsoft. It uses an annotations.json file to organize image data. This json annotation approach has become standard for other datasets to use.\n",
        "\n",
        "Lots of datasets use COCO-style of formatting. In addition to the training images themselves, these datasets have at least one annotations json file which contains the following:\n",
        "*   \"images\":  List of image metadata\n",
        "*   \"annotations\":  List of label data (type of trash, bounding box definition, segmentation data; corresponds to images list)\n",
        "*   \"categories\":  List of the different categories this dataset uses\n",
        "\n",
        "An example of a COCO-style dataset file structure is as follows:\n",
        "```\n",
        "dataset/\n",
        "├── annotations/\n",
        "│   ├── instances_train2017.json\n",
        "│   ├── instances_val2017.json\n",
        "│   ├── person_keypoints_train2017.json\n",
        "│   ├── captions_train2017.json\n",
        "│   └── ... (other task-specific .json files)\n",
        "├── images/\n",
        "│   ├── train2017/\n",
        "│   │   ├── 000000000009.jpg\n",
        "│   │   ├── 000000000025.jpg\n",
        "│   │   └── ...\n",
        "│   └── val2017/\n",
        "│       ├── 000000000139.jpg\n",
        "│       ├── 000000000285.jpg\n",
        "│       └── ...\n",
        "└── LICENSE.txt (optional)\n",
        "```\n",
        "\n",
        "## COCO format required by the COCO-to-YOLO conversion function\n",
        "\n",
        "\n",
        "To use the COCO to YOLO conversion function below, your data set must conform to the following (vastly simplified) COCO-like directory structure and json structure. It's unlikely that your dataset will conform to these specifications out of the box, so please use the code section below to modify your data's structure to match.\n",
        "\n",
        "The COCO directory structure must be as follows, with a folder called \"dataset\" located in your \"content\" directory:\n",
        "```\n",
        "dataset/\n",
        "├── images/\n",
        "│   ├── 000001.jpg  # or png or whatever\n",
        "│   ├── 000002.jpg\n",
        "│   └── ...\n",
        "└── annotations.json\n",
        "```\n",
        "\n",
        "And the annotations.json file must contain information in the following structure, and using the following json keywords:\n",
        "```\n",
        "{\n",
        "  \"images\": [\n",
        "    {\n",
        "      \"id\": 1,\n",
        "      \"file_name\": \"000001.jpg\",\n",
        "      \"width\": 640,\n",
        "      \"height\": 480\n",
        "    },\n",
        "    {\n",
        "      \"id\": 2,\n",
        "      \"file_name\": \"000002.jpg\",\n",
        "      \"width\": 800,\n",
        "      \"height\": 600\n",
        "    }\n",
        "  ],\n",
        "  \"annotations\": [\n",
        "    {\n",
        "      \"id\": 1,\n",
        "      \"image_id\": 1,\n",
        "      \"category_id\": 1,\n",
        "      \"bbox\": [100, 120, 50, 60],\n",
        "      \"area\": 3000,\n",
        "      \"iscrowd\": 0\n",
        "    },\n",
        "    {\n",
        "      \"id\": 2,\n",
        "      \"image_id\": 2,\n",
        "      \"category_id\": 2,\n",
        "      \"bbox\": [20, 30, 40, 50],\n",
        "      \"area\": 2000,\n",
        "      \"iscrowd\": 0\n",
        "    }\n",
        "  ],\n",
        "  \"categories\": [\n",
        "    {\n",
        "      \"id\": 1,\n",
        "      \"name\": \"plastic\"\n",
        "    },\n",
        "    {\n",
        "      \"id\": 2,\n",
        "      \"name\": \"metal\"\n",
        "    }\n",
        "  ]\n",
        "}\n",
        "```\n"
      ],
      "metadata": {
        "id": "wKqkU54arQMg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Helpful functions\n",
        "\n",
        "This section has some helpful functions you can use later in this notebook."
      ],
      "metadata": {
        "id": "epBrA9Nccr7u"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "from pprint import pprint\n",
        "\n",
        "def show_first_two_per_category(json_path):\n",
        "    \"\"\"\n",
        "    Prints the first two entries of each root-level list in a JSON file.\n",
        "\n",
        "    Useful for quickly inspecting the structure and content of an COCO-style\n",
        "    annotations json file.\n",
        "\n",
        "    It pretty-prints the first two entries of each top-level key that contains a list.\n",
        "\n",
        "    Args:\n",
        "        json_path (str or Path): Path to the JSON file to inspect.\n",
        "\n",
        "    Raises:\n",
        "        FileNotFoundError: If the provided path does not point to an existing file.\n",
        "        json.JSONDecodeError: If the file is not valid JSON.\n",
        "    \"\"\"\n",
        "    json_path = Path(json_path)\n",
        "\n",
        "    if not json_path.exists():\n",
        "        print(f\"File not found: {json_path}\")\n",
        "        return\n",
        "\n",
        "    with open(json_path, 'r') as f:\n",
        "        data = json.load(f)\n",
        "\n",
        "    for key, value in data.items():\n",
        "        print(f\"\\n--- {key.upper()} (showing first 2 entries) ---\")\n",
        "        if isinstance(value, list):\n",
        "            for item in value[:2]:\n",
        "                pprint(item)\n",
        "        else:\n",
        "            print(f\"{key} is not a list, skipping.\")"
      ],
      "metadata": {
        "id": "CgZ9lFw7bpLv"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "---\n",
        "⚠️ ***CHANGE THIS FILE FROM HERE DOWN TO SUIT YOUR DATASET*** ⚠️\n",
        "\n",
        "The sections above apply for all dataset conversions.\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "GUP0h7VX_seR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Sometimes this has to be run twice for the data to show up in the file tree\n",
        "import kagglehub\n",
        "from pathlib import Path\n",
        "\n",
        "dataset_name = \"YOLO\"\n",
        "taco_dataset_path = Path(kagglehub.dataset_download('kneroma/tacotrashdataset'))  # https://www.kaggle.com/datasets/kneroma/tacotrashdataset\n",
        "print(f\"{dataset_name} dataset downloaded to {taco_dataset_path}\\n\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DwT18ruVap0W",
        "outputId": "958ce8cf-665b-4a57-8a41-815fc5c555c8"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset downloaded to /kaggle/input/tacotrashdataset\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# If your dataset has one or more annotations json files, inspect them to help discover some of the quirks your dataset has.\n",
        "show_first_two_per_category(taco_dataset_path / \"data\" / \"annotations.json\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r4jKmumvccnN",
        "outputId": "7bd37408-7d21-4e17-9853-b715813ba77d"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- INFO (showing first 2 entries) ---\n",
            "info is not a list, skipping.\n",
            "\n",
            "--- IMAGES (showing first 2 entries) ---\n",
            "{'coco_url': None,\n",
            " 'date_captured': None,\n",
            " 'file_name': 'batch_1/000006.jpg',\n",
            " 'flickr_640_url': 'https://farm66.staticflickr.com/65535/33978196618_632623b4fc_z.jpg',\n",
            " 'flickr_url': 'https://farm66.staticflickr.com/65535/33978196618_e30a59e0a8_o.png',\n",
            " 'height': 2049,\n",
            " 'id': 0,\n",
            " 'license': None,\n",
            " 'width': 1537}\n",
            "{'coco_url': None,\n",
            " 'date_captured': None,\n",
            " 'file_name': 'batch_1/000008.jpg',\n",
            " 'flickr_640_url': 'https://farm66.staticflickr.com/65535/47803331152_19beae025a_z.jpg',\n",
            " 'flickr_url': 'https://farm66.staticflickr.com/65535/47803331152_ee00755a2e_o.png',\n",
            " 'height': 2049,\n",
            " 'id': 1,\n",
            " 'license': None,\n",
            " 'width': 1537}\n",
            "\n",
            "--- ANNOTATIONS (showing first 2 entries) ---\n",
            "{'area': 403954.0,\n",
            " 'bbox': [517.0, 127.0, 447.0, 1322.0],\n",
            " 'category_id': 6,\n",
            " 'id': 1,\n",
            " 'image_id': 0,\n",
            " 'iscrowd': 0,\n",
            " 'segmentation': [[561.0,\n",
            "                   1238.0,\n",
            "                   568.0,\n",
            "                   1201.0,\n",
            "                   567.0,\n",
            "                   1175.0,\n",
            "                   549.0,\n",
            "                   1127.0,\n",
            "                   538.0,\n",
            "                   1089.0,\n",
            "                   519.0,\n",
            "                   1043.0,\n",
            "                   517.0,\n",
            "                   1005.0,\n",
            "                   523.0,\n",
            "                   964.0,\n",
            "                   529.0,\n",
            "                   945.0,\n",
            "                   520.0,\n",
            "                   896.0,\n",
            "                   525.0,\n",
            "                   862.0,\n",
            "                   536.0,\n",
            "                   821.0,\n",
            "                   554.0,\n",
            "                   769.0,\n",
            "                   577.0,\n",
            "                   727.0,\n",
            "                   595.0,\n",
            "                   678.0,\n",
            "                   596.0,\n",
            "                   585.0,\n",
            "                   588.0,\n",
            "                   346.0,\n",
            "                   581.0,\n",
            "                   328.0,\n",
            "                   569.0,\n",
            "                   306.0,\n",
            "                   570.0,\n",
            "                   276.0,\n",
            "                   576.0,\n",
            "                   224.0,\n",
            "                   560.0,\n",
            "                   205.0,\n",
            "                   564.0,\n",
            "                   170.0,\n",
            "                   578.0,\n",
            "                   154.0,\n",
            "                   608.0,\n",
            "                   136.0,\n",
            "                   649.0,\n",
            "                   127.0,\n",
            "                   688.0,\n",
            "                   127.0,\n",
            "                   726.0,\n",
            "                   129.0,\n",
            "                   759.0,\n",
            "                   141.0,\n",
            "                   784.0,\n",
            "                   153.0,\n",
            "                   792.0,\n",
            "                   177.0,\n",
            "                   788.0,\n",
            "                   193.0,\n",
            "                   782.0,\n",
            "                   209.0,\n",
            "                   792.0,\n",
            "                   238.0,\n",
            "                   802.0,\n",
            "                   271.0,\n",
            "                   802.0,\n",
            "                   294.0,\n",
            "                   791.0,\n",
            "                   319.0,\n",
            "                   789.0,\n",
            "                   360.0,\n",
            "                   794.0,\n",
            "                   395.0,\n",
            "                   810.0,\n",
            "                   529.0,\n",
            "                   819.0,\n",
            "                   609.0,\n",
            "                   841.0,\n",
            "                   675.0,\n",
            "                   882.0,\n",
            "                   728.0,\n",
            "                   916.0,\n",
            "                   781.0,\n",
            "                   928.0,\n",
            "                   802.0,\n",
            "                   938.0,\n",
            "                   834.0,\n",
            "                   940.0,\n",
            "                   856.0,\n",
            "                   939.0,\n",
            "                   873.0,\n",
            "                   938.0,\n",
            "                   884.0,\n",
            "                   944.0,\n",
            "                   901.0,\n",
            "                   951.0,\n",
            "                   917.0,\n",
            "                   956.0,\n",
            "                   942.0,\n",
            "                   960.0,\n",
            "                   972.0,\n",
            "                   964.0,\n",
            "                   1013.0,\n",
            "                   959.0,\n",
            "                   1036.0,\n",
            "                   952.0,\n",
            "                   1081.0,\n",
            "                   952.0,\n",
            "                   1106.0,\n",
            "                   934.0,\n",
            "                   1163.0,\n",
            "                   935.0,\n",
            "                   1174.0,\n",
            "                   949.0,\n",
            "                   1209.0,\n",
            "                   954.0,\n",
            "                   1235.0,\n",
            "                   952.0,\n",
            "                   1273.0,\n",
            "                   953.0,\n",
            "                   1296.0,\n",
            "                   946.0,\n",
            "                   1320.0,\n",
            "                   930.0,\n",
            "                   1347.0,\n",
            "                   914.0,\n",
            "                   1367.0,\n",
            "                   898.0,\n",
            "                   1388.0,\n",
            "                   855.0,\n",
            "                   1419.0,\n",
            "                   821.0,\n",
            "                   1437.0,\n",
            "                   769.0,\n",
            "                   1449.0,\n",
            "                   728.0,\n",
            "                   1448.0,\n",
            "                   687.0,\n",
            "                   1439.0,\n",
            "                   645.0,\n",
            "                   1419.0,\n",
            "                   618.0,\n",
            "                   1397.0,\n",
            "                   582.0,\n",
            "                   1348.0,\n",
            "                   572.0,\n",
            "                   1314.0,\n",
            "                   564.0,\n",
            "                   1279.0,\n",
            "                   561.0,\n",
            "                   1253.0,\n",
            "                   561.0,\n",
            "                   1238.0]]}\n",
            "{'area': 1071259.5,\n",
            " 'bbox': [1.0, 457.0, 1429.0, 1519.0],\n",
            " 'category_id': 18,\n",
            " 'id': 2,\n",
            " 'image_id': 1,\n",
            " 'iscrowd': 0,\n",
            " 'segmentation': [[928.0,\n",
            "                   1876.0,\n",
            "                   938.0,\n",
            "                   1856.0,\n",
            "                   968.0,\n",
            "                   1826.0,\n",
            "                   990.0,\n",
            "                   1808.0,\n",
            "                   998.0,\n",
            "                   1790.0,\n",
            "                   1069.0,\n",
            "                   1727.0,\n",
            "                   1096.0,\n",
            "                   1702.0,\n",
            "                   1159.0,\n",
            "                   1644.0,\n",
            "                   1212.0,\n",
            "                   1588.0,\n",
            "                   1258.0,\n",
            "                   1540.0,\n",
            "                   1314.0,\n",
            "                   1482.0,\n",
            "                   1357.0,\n",
            "                   1444.0,\n",
            "                   1392.0,\n",
            "                   1416.0,\n",
            "                   1409.0,\n",
            "                   1393.0,\n",
            "                   1430.0,\n",
            "                   1369.0,\n",
            "                   1415.0,\n",
            "                   1347.0,\n",
            "                   1130.0,\n",
            "                   1087.0,\n",
            "                   780.0,\n",
            "                   763.0,\n",
            "                   528.0,\n",
            "                   533.0,\n",
            "                   479.0,\n",
            "                   486.0,\n",
            "                   466.0,\n",
            "                   466.0,\n",
            "                   448.0,\n",
            "                   457.0,\n",
            "                   427.0,\n",
            "                   468.0,\n",
            "                   387.0,\n",
            "                   502.0,\n",
            "                   321.0,\n",
            "                   554.0,\n",
            "                   244.0,\n",
            "                   608.0,\n",
            "                   118.0,\n",
            "                   693.0,\n",
            "                   37.0,\n",
            "                   750.0,\n",
            "                   3.0,\n",
            "                   780.0,\n",
            "                   1.0,\n",
            "                   995.0,\n",
            "                   28.0,\n",
            "                   1032.0,\n",
            "                   104.0,\n",
            "                   1119.0,\n",
            "                   403.0,\n",
            "                   1471.0,\n",
            "                   666.0,\n",
            "                   1805.0,\n",
            "                   763.0,\n",
            "                   1954.0,\n",
            "                   782.0,\n",
            "                   1945.0,\n",
            "                   796.0,\n",
            "                   1970.0,\n",
            "                   803.0,\n",
            "                   1976.0,\n",
            "                   818.0,\n",
            "                   1976.0,\n",
            "                   836.0,\n",
            "                   1956.0,\n",
            "                   852.0,\n",
            "                   1954.0,\n",
            "                   860.0,\n",
            "                   1937.0,\n",
            "                   873.0,\n",
            "                   1931.0,\n",
            "                   885.0,\n",
            "                   1908.0,\n",
            "                   898.0,\n",
            "                   1896.0,\n",
            "                   928.0,\n",
            "                   1876.0]]}\n",
            "\n",
            "--- SCENE_ANNOTATIONS (showing first 2 entries) ---\n",
            "{'background_ids': [1], 'image_id': 0}\n",
            "{'background_ids': [1], 'image_id': 1}\n",
            "\n",
            "--- LICENSES (showing first 2 entries) ---\n",
            "\n",
            "--- CATEGORIES (showing first 2 entries) ---\n",
            "{'id': 0, 'name': 'Aluminium foil', 'supercategory': 'Aluminium foil'}\n",
            "{'id': 1, 'name': 'Battery', 'supercategory': 'Battery'}\n",
            "\n",
            "--- SCENE_CATEGORIES (showing first 2 entries) ---\n",
            "{'id': 0, 'name': 'Clean'}\n",
            "{'id': 1, 'name': 'Indoor, Man-made'}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# TACO dataset\n",
        "\n",
        "The TACO dataset uses COCO-style formatting (segmentation).\n",
        "\n",
        "## Quirks\n",
        "\n",
        "TACO has 15 batch folders, each containing jpgs with names like 000000.jpg, 000001.jpg, 000002.jpg, etc.\n",
        "* Some numbers are skipped, eg 000000.jpg, 000001.jpg, 000003.jpg.\n",
        "* The name 000000.jpg is used in multiple batch folders, to name different image files.\n",
        "* It contains segmentation data (will be discarded).\n",
        "* It uses categories (specific) and supercategories (more general) - only the supercategories will be retained.\n",
        "* It includes scene categories (eg clearn streets) - this will be discarded.\n",
        "  * It might be interesting in the future to consider using a multi-tasking model - detecting trash and also classsifying the scene might improve the models performance by adding a bit of context awareness.\n",
        "\n",
        "## Conversion\n",
        "To convert the TACO dataset to a format ultralytics YOLO can use, we must:\n",
        "* Give the images unique names\n",
        "* Split the TACO images into train and val sets\n",
        "* Extract label and bbox info from annotations.json, and save it in individual txt files corresponding to the image files\n",
        "* Make a data.yaml file"
      ],
      "metadata": {
        "id": "f7u6PWyfxeLD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Modify your dataset structure to match the required simple-COCO style\n",
        "import shutil\n",
        "\n",
        "source_root = taco_dataset_path / \"data\"\n",
        "annotations_path = source_root / 'annotations.json'\n",
        "\n",
        "target_root = Path('/content/dataset')\n",
        "target_img_dir = target_root / 'images'\n",
        "if target_root.exists():\n",
        "    shutil.rmtree(target_root)\n",
        "target_img_dir.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "with annotations_path.open('r') as f:\n",
        "    annotations = json.load(f)\n",
        "\n",
        "# Reassign file names with unique zero-padded numbering\n",
        "image_id_map = {}\n",
        "for idx, image_info in enumerate(annotations['images']):\n",
        "    old_path = source_root / image_info['file_name']\n",
        "    new_filename = f\"{idx:06}.jpg\"\n",
        "    new_path = target_img_dir / new_filename\n",
        "\n",
        "    image_id_map[old_path] = new_path\n",
        "    image_info['file_name'] = new_filename\n",
        "\n",
        "    # Keep only required image fields\n",
        "    annotations['images'][idx] = {\n",
        "        'id': image_info['id'],\n",
        "        'file_name': new_filename,\n",
        "        'width': image_info['width'],\n",
        "        'height': image_info['height']\n",
        "    }\n",
        "\n",
        "# Prune annotation fields\n",
        "for ann in annotations['annotations']:\n",
        "    ann_keys = ['id', 'image_id', 'category_id', 'bbox', 'area', 'iscrowd']\n",
        "    for key in list(ann.keys()):\n",
        "        if key not in ann_keys:\n",
        "            del ann[key]\n",
        "\n",
        "# Update category entries to only include id and name = supercategory\n",
        "for cat in annotations['categories']:\n",
        "    cat_id = cat['id']\n",
        "    cat_super = cat.get('supercategory', f'category_{cat_id}')\n",
        "    cat.clear()\n",
        "    cat['id'] = cat_id\n",
        "    cat['name'] = cat_super\n",
        "\n",
        "# Copy image files to new location\n",
        "for src, dst in image_id_map.items():\n",
        "    if src.exists():\n",
        "        shutil.copy(src, dst)\n",
        "    else:\n",
        "        print(f\"Warning: Missing image: {src}\")\n",
        "\n",
        "# Remove unwanted top-level keys\n",
        "for key in ['info', 'scene_annotations', 'scene_categories', 'licenses']:\n",
        "    annotations.pop(key, None)\n",
        "\n",
        "# Save cleaned annotations\n",
        "new_annotations_path = target_root / 'annotations.json'\n",
        "with new_annotations_path.open('w') as f:\n",
        "    json.dump(annotations, f)\n",
        "\n",
        "print(f\"Converted and cleaned {len(image_id_map)} images.\")"
      ],
      "metadata": {
        "id": "Usb6nID5YC8i",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "06f7fc44-3036-430e-8dc5-5ddb74756f28"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Converted and cleaned 1500 images.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Inspect your new annotations.json file to make sure the format and field names match the simple COCO format described above\n",
        "show_first_two_per_category(target_root / \"annotations.json\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EB3t3Ey-hq9E",
        "outputId": "c1e24dfb-3ca5-4327-e9dd-ab77a67dfc6f"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- IMAGES (showing first 2 entries) ---\n",
            "{'file_name': '000000.jpg', 'height': 2049, 'id': 0, 'width': 1537}\n",
            "{'file_name': '000001.jpg', 'height': 2049, 'id': 1, 'width': 1537}\n",
            "\n",
            "--- ANNOTATIONS (showing first 2 entries) ---\n",
            "{'area': 403954.0,\n",
            " 'bbox': [517.0, 127.0, 447.0, 1322.0],\n",
            " 'category_id': 6,\n",
            " 'id': 1,\n",
            " 'image_id': 0,\n",
            " 'iscrowd': 0}\n",
            "{'area': 1071259.5,\n",
            " 'bbox': [1.0, 457.0, 1429.0, 1519.0],\n",
            " 'category_id': 18,\n",
            " 'id': 2,\n",
            " 'image_id': 1,\n",
            " 'iscrowd': 0}\n",
            "\n",
            "--- CATEGORIES (showing first 2 entries) ---\n",
            "{'id': 0, 'name': 'Aluminium foil'}\n",
            "{'id': 1, 'name': 'Battery'}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import shutil\n",
        "import random\n",
        "import yaml\n",
        "from collections import defaultdict\n",
        "\n",
        "def convert_coco_to_yolo(coco_root: Path, dataset_name: str, train_split: float = 0.8):\n",
        "    \"\"\"\n",
        "    Converts a COCO-style dataset to YOLOv8 format, including train/val split and data.yaml generation.\n",
        "\n",
        "    Args:\n",
        "        coco_root (Path): Path to the root of the COCO-style dataset (should contain images/ and annotations.json).\n",
        "        dataset_name (str): Name of the output dataset folder (e.g., \"taco\" -> creates \"taco_yolo\").\n",
        "        train_split (float, optional): Fraction of images to use for training. Defaults to 0.8.\n",
        "          The remaining images are split between validation and testing.\n",
        "\n",
        "    Returns:\n",
        "        Path: Path to the data.yaml file\n",
        "    \"\"\"\n",
        "    # Paths\n",
        "    coco_json_path = coco_root / 'annotations.json'\n",
        "    coco_images_path = coco_root / 'images'\n",
        "    yolo_root = coco_root.parent / f\"{dataset_name}_yolo\"\n",
        "    yolo_img_dirs = {\n",
        "        'train': yolo_root / 'images' / 'train',\n",
        "        'val': yolo_root / 'images' / 'val',\n",
        "        'test': yolo_root / 'images' / 'test',\n",
        "    }\n",
        "    yolo_lbl_dirs = {\n",
        "        'train': yolo_root / 'labels' / 'train',\n",
        "        'val': yolo_root / 'labels' / 'val',\n",
        "        'test': yolo_root / 'labels' / 'test',\n",
        "    }\n",
        "\n",
        "    # Clear and recreate folders\n",
        "    for d in list(yolo_img_dirs.values()) + list(yolo_lbl_dirs.values()):\n",
        "        if d.exists():\n",
        "            shutil.rmtree(d)\n",
        "        d.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "    # Load COCO JSON\n",
        "    with open(coco_json_path, 'r') as f:\n",
        "        coco = json.load(f)\n",
        "\n",
        "    # Map image_id -> metadata\n",
        "    image_info = {img['id']: (img['width'], img['height'], img['file_name']) for img in coco['images']}\n",
        "\n",
        "    # Map image_id -> annotations\n",
        "    annots_per_image = defaultdict(list)\n",
        "    for ann in coco['annotations']:\n",
        "        annots_per_image[ann['image_id']].append(ann)\n",
        "\n",
        "    # Shuffle and split image IDs\n",
        "    all_image_ids = list(image_info.keys())\n",
        "    random.shuffle(all_image_ids)\n",
        "    n_total = len(all_image_ids)\n",
        "    n_train = int(n_total * train_split)\n",
        "    n_val = int((n_total - n_train) / 2)\n",
        "    n_test = n_total - n_train - n_val\n",
        "\n",
        "    split_ids = {\n",
        "        'train': set(all_image_ids[:n_train]),\n",
        "        'val': set(all_image_ids[n_train:n_train + n_val]),\n",
        "        'test': set(all_image_ids[n_train + n_val:]),\n",
        "    }\n",
        "\n",
        "    def write_labels_and_copy_images(image_ids, img_dir, lbl_dir):\n",
        "        for image_id in image_ids:\n",
        "            width, height, filename = image_info[image_id]\n",
        "            orig_stem = Path(filename).stem\n",
        "            new_stem = f\"{dataset_name}_{orig_stem}\"\n",
        "            label_path = lbl_dir / f\"{new_stem}.txt\"\n",
        "            image_src = coco_images_path / filename\n",
        "            image_dst = img_dir / f\"{new_stem}.jpg\"\n",
        "\n",
        "            # Copy image\n",
        "            if image_src.exists():\n",
        "                shutil.copy(image_src, image_dst)\n",
        "            else:\n",
        "                print(f\"Warning: Image not found: {image_src}\")\n",
        "                continue\n",
        "\n",
        "            # Write labels\n",
        "            with open(label_path, 'w') as f:\n",
        "                for ann in annots_per_image.get(image_id, []):\n",
        "                    class_id = ann['category_id']\n",
        "                    x, y, w, h = ann['bbox']\n",
        "                    x_center = (x + w / 2) / width\n",
        "                    y_center = (y + h / 2) / height\n",
        "                    w /= width\n",
        "                    h /= height\n",
        "                    f.write(f\"{class_id} {x_center:.6f} {y_center:.6f} {w:.6f} {h:.6f}\\n\")\n",
        "\n",
        "    # Process splits\n",
        "    for split in ['train', 'val', 'test']:\n",
        "        write_labels_and_copy_images(\n",
        "            split_ids[split],\n",
        "            yolo_img_dirs[split],\n",
        "            yolo_lbl_dirs[split]\n",
        "        )\n",
        "\n",
        "    # Build data.yaml\n",
        "    categories = sorted(coco['categories'], key=lambda x: x['id'])\n",
        "    names = [cat['name'] for cat in categories]\n",
        "    data_yaml = {\n",
        "        'path': str(yolo_root),\n",
        "        'train': 'images/train',\n",
        "        'val': 'images/val',\n",
        "        'test': 'images/test',\n",
        "        'nc': len(names),\n",
        "        'names': names\n",
        "    }\n",
        "\n",
        "    with open(yolo_root / 'data.yaml', 'w') as f:\n",
        "        yaml.dump(data_yaml, f)\n",
        "\n",
        "    print(f\"YOLO conversion complete: {yolo_root}\")\n",
        "    print(f\"  Train: {len(split_ids['train'])}, Val: {len(split_ids['val'])}, Test: {len(split_ids['test'])}\")\n",
        "    return yolo_root / 'data.yaml'"
      ],
      "metadata": {
        "id": "w0VITl3VnG_n"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "yolo_data = convert_coco_to_yolo(target_root, dataset_name)"
      ],
      "metadata": {
        "id": "L_uawwBmoB2u",
        "outputId": "b48c3b5d-3b29-48be-e89a-506b42157090",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "YOLO conversion complete: /content/TACO_yolo\n",
            "  Train: 1200, Val: 150, Test: 150\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "To verify that your conversion worked, make sure you can train a model and that it outputs images with a bounding box and label."
      ],
      "metadata": {
        "id": "xpsMcME2-rDs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -U ultralytics\n",
        "\n",
        "from ultralytics import YOLO\n",
        "import os"
      ],
      "metadata": {
        "collapsed": true,
        "id": "Mb5g0TegvNCl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = YOLO('yolo11n.pt')\n",
        "results = model.train(data=str(yolo_data), epochs=2, imgsz=640)  # epoch size is small - this is just to see if it can work!"
      ],
      "metadata": {
        "collapsed": true,
        "id": "UL4rIlXU-oLt",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "df6da521-0308-4cd5-ab6c-eee28f48a7c2"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Ultralytics 8.3.148 🚀 Python-3.11.12 torch-2.6.0+cu124 CPU (Intel Xeon 2.20GHz)\n",
            "\u001b[34m\u001b[1mengine/trainer: \u001b[0magnostic_nms=False, amp=True, augment=False, auto_augment=randaugment, batch=16, bgr=0.0, box=7.5, cache=False, cfg=None, classes=None, close_mosaic=10, cls=0.5, conf=None, copy_paste=0.0, copy_paste_mode=flip, cos_lr=False, cutmix=0.0, data=/content/TACO_yolo/data.yaml, degrees=0.0, deterministic=True, device=cpu, dfl=1.5, dnn=False, dropout=0.0, dynamic=False, embed=None, epochs=2, erasing=0.4, exist_ok=False, fliplr=0.5, flipud=0.0, format=torchscript, fraction=1.0, freeze=None, half=False, hsv_h=0.015, hsv_s=0.7, hsv_v=0.4, imgsz=640, int8=False, iou=0.7, keras=False, kobj=1.0, line_width=None, lr0=0.01, lrf=0.01, mask_ratio=4, max_det=300, mixup=0.0, mode=train, model=yolo11n.pt, momentum=0.937, mosaic=1.0, multi_scale=False, name=train2, nbs=64, nms=False, opset=None, optimize=False, optimizer=auto, overlap_mask=True, patience=100, perspective=0.0, plots=True, pose=12.0, pretrained=True, profile=False, project=None, rect=False, resume=False, retina_masks=False, save=True, save_conf=False, save_crop=False, save_dir=runs/detect/train2, save_frames=False, save_json=False, save_period=-1, save_txt=False, scale=0.5, seed=0, shear=0.0, show=False, show_boxes=True, show_conf=True, show_labels=True, simplify=True, single_cls=False, source=None, split=val, stream_buffer=False, task=detect, time=None, tracker=botsort.yaml, translate=0.1, val=True, verbose=True, vid_stride=1, visualize=False, warmup_bias_lr=0.1, warmup_epochs=3.0, warmup_momentum=0.8, weight_decay=0.0005, workers=8, workspace=None\n",
            "Overriding model.yaml nc=80 with nc=60\n",
            "\n",
            "                   from  n    params  module                                       arguments                     \n",
            "  0                  -1  1       464  ultralytics.nn.modules.conv.Conv             [3, 16, 3, 2]                 \n",
            "  1                  -1  1      4672  ultralytics.nn.modules.conv.Conv             [16, 32, 3, 2]                \n",
            "  2                  -1  1      6640  ultralytics.nn.modules.block.C3k2            [32, 64, 1, False, 0.25]      \n",
            "  3                  -1  1     36992  ultralytics.nn.modules.conv.Conv             [64, 64, 3, 2]                \n",
            "  4                  -1  1     26080  ultralytics.nn.modules.block.C3k2            [64, 128, 1, False, 0.25]     \n",
            "  5                  -1  1    147712  ultralytics.nn.modules.conv.Conv             [128, 128, 3, 2]              \n",
            "  6                  -1  1     87040  ultralytics.nn.modules.block.C3k2            [128, 128, 1, True]           \n",
            "  7                  -1  1    295424  ultralytics.nn.modules.conv.Conv             [128, 256, 3, 2]              \n",
            "  8                  -1  1    346112  ultralytics.nn.modules.block.C3k2            [256, 256, 1, True]           \n",
            "  9                  -1  1    164608  ultralytics.nn.modules.block.SPPF            [256, 256, 5]                 \n",
            " 10                  -1  1    249728  ultralytics.nn.modules.block.C2PSA           [256, 256, 1]                 \n",
            " 11                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
            " 12             [-1, 6]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
            " 13                  -1  1    111296  ultralytics.nn.modules.block.C3k2            [384, 128, 1, False]          \n",
            " 14                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
            " 15             [-1, 4]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
            " 16                  -1  1     32096  ultralytics.nn.modules.block.C3k2            [256, 64, 1, False]           \n",
            " 17                  -1  1     36992  ultralytics.nn.modules.conv.Conv             [64, 64, 3, 2]                \n",
            " 18            [-1, 13]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
            " 19                  -1  1     86720  ultralytics.nn.modules.block.C3k2            [192, 128, 1, False]          \n",
            " 20                  -1  1    147712  ultralytics.nn.modules.conv.Conv             [128, 128, 3, 2]              \n",
            " 21            [-1, 10]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
            " 22                  -1  1    378880  ultralytics.nn.modules.block.C3k2            [384, 256, 1, True]           \n",
            " 23        [16, 19, 22]  1    442372  ultralytics.nn.modules.head.Detect           [60, [64, 128, 256]]          \n",
            "YOLO11n summary: 181 layers, 2,601,540 parameters, 2,601,524 gradients, 6.5 GFLOPs\n",
            "\n",
            "Transferred 448/499 items from pretrained weights\n",
            "Freezing layer 'model.23.dfl.conv.weight'\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0mFast image access ✅ (ping: 0.0±0.0 ms, read: 1356.7±685.8 MB/s, size: 2153.1 KB)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mtrain: \u001b[0mScanning /content/TACO_yolo/labels/train... 1200 images, 0 backgrounds, 0 corrupt: 100%|██████████| 1200/1200 [00:01<00:00, 821.98it/s] "
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[34m\u001b[1mtrain: \u001b[0mNew cache created: /content/TACO_yolo/labels/train.cache\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[34m\u001b[1malbumentations: \u001b[0mBlur(p=0.01, blur_limit=(3, 7)), MedianBlur(p=0.01, blur_limit=(3, 7)), ToGray(p=0.01, method='weighted_average', num_output_channels=3), CLAHE(p=0.01, clip_limit=(1.0, 4.0), tile_grid_size=(8, 8))\n",
            "\u001b[34m\u001b[1mval: \u001b[0mFast image access ✅ (ping: 0.0±0.0 ms, read: 2295.6±269.7 MB/s, size: 1323.9 KB)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mval: \u001b[0mScanning /content/TACO_yolo/labels/val... 150 images, 0 backgrounds, 0 corrupt: 100%|██████████| 150/150 [00:00<00:00, 1728.78it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[34m\u001b[1mval: \u001b[0mNew cache created: /content/TACO_yolo/labels/val.cache\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Plotting labels to runs/detect/train2/labels.jpg... \n",
            "\u001b[34m\u001b[1moptimizer:\u001b[0m 'optimizer=auto' found, ignoring 'lr0=0.01' and 'momentum=0.937' and determining best 'optimizer', 'lr0' and 'momentum' automatically... \n",
            "\u001b[34m\u001b[1moptimizer:\u001b[0m AdamW(lr=0.000156, momentum=0.9) with parameter groups 81 weight(decay=0.0), 88 weight(decay=0.0005), 87 bias(decay=0.0)\n",
            "Image sizes 640 train, 640 val\n",
            "Using 0 dataloader workers\n",
            "Logging results to \u001b[1mruns/detect/train2\u001b[0m\n",
            "Starting training for 2 epochs...\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "        1/2         0G      1.302      5.498      1.272         89        640:  28%|██▊       | 21/75 [06:18<16:13, 18.02s/it]\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-28-4580efae8ac8>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mYOLO\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'yolo11n.pt'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0myolo_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimgsz\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m640\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# epoch size is small - this is just to see if it can work!\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/ultralytics/engine/model.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, trainer, **kwargs)\u001b[0m\n\u001b[1;32m    795\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    796\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhub_session\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msession\u001b[0m  \u001b[0;31m# attach optional HUB session\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 797\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    798\u001b[0m         \u001b[0;31m# Update model and cfg after training\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    799\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mRANK\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/ultralytics/engine/trainer.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    225\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    226\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 227\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_train\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mworld_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    228\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    229\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_setup_scheduler\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/ultralytics/engine/trainer.py\u001b[0m in \u001b[0;36m_do_train\u001b[0;34m(self, world_size)\u001b[0m\n\u001b[1;32m    413\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    414\u001b[0m                 \u001b[0;31m# Backward\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 415\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscaler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscale\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    416\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    417\u001b[0m                 \u001b[0;31m# Optimize - https://pytorch.org/docs/master/notes/amp_examples.html\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    624\u001b[0m                 \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    625\u001b[0m             )\n\u001b[0;32m--> 626\u001b[0;31m         torch.autograd.backward(\n\u001b[0m\u001b[1;32m    627\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    628\u001b[0m         )\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    345\u001b[0m     \u001b[0;31m# some Python versions print out the first line of a multi-line function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    346\u001b[0m     \u001b[0;31m# calls in the traceback and some print out the last line\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 347\u001b[0;31m     _engine_run_backward(\n\u001b[0m\u001b[1;32m    348\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    349\u001b[0m         \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/autograd/graph.py\u001b[0m in \u001b[0;36m_engine_run_backward\u001b[0;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    821\u001b[0m         \u001b[0munregister_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_register_logging_hooks_on_whole_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt_outputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    822\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 823\u001b[0;31m         return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n\u001b[0m\u001b[1;32m    824\u001b[0m             \u001b[0mt_outputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    825\u001b[0m         )  # Calls into the C++ engine to run the backward pass\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "If the model outputs even one image with a bounding box and label, then the dataset should work for our project! Verify this using the code below."
      ],
      "metadata": {
        "id": "2JPNfPNAEtFK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import cv2\n",
        "from random import sample\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "model = YOLO('runs/detect/train/weights/best.pt')\n",
        "train_images_path = yolo_data.parent / \"images\" / \"train\")\n",
        "image_files = list(train_images_path.glob('*.jpg'))\n",
        "\n",
        "sample_images = sample(image_files, 10)\n",
        "\n",
        "for image_path in sample_images:\n",
        "    result = model(image_path)[0]\n",
        "    annotated_image = result.plot()\n",
        "\n",
        "    plt.figure(figsize=(8, 6))\n",
        "    plt.imshow(annotated_image)\n",
        "    plt.title(f'Predictions: {image_path.name}')\n",
        "    plt.axis('off')\n",
        "    plt.show()"
      ],
      "metadata": {
        "collapsed": true,
        "id": "Apy8pfX6Eyhe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "If the model successfully generated even one image with a bounding box and label, please run the following code block to zip the taco_yolo dataset, download the zipped file, and upload it on Google Drive, https://www.google.com/url?q=https%3A%2F%2Fdrive.google.com%2Fdrive%2Ffolders%2F1bUkIYQRXX08OKI5TuOSg-eqntSudGaFB."
      ],
      "metadata": {
        "id": "zj5c-mtlL2gk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from datetime import datetime\n",
        "\n",
        "# Generate date prefix\n",
        "date_str = datetime.now().strftime('%Y%m%d')\n",
        "zip_name = f\"{date_str}_{dataset_name}.zip\"\n",
        "\n",
        "# Change directory and zip\n",
        "%cd /kaggle/working/{dataset_name}\n",
        "!zip -r /content/{zip_name} .\n",
        "\n",
        "print(f\"Zip created at /content/{zip_name}\")"
      ],
      "metadata": {
        "collapsed": true,
        "id": "Ik8EKrBMLyap"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}