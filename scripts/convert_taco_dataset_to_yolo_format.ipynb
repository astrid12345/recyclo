{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMe2KR5lSgmZcAIuVwf0H8O",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/astrid12345/recyclo/blob/convert_taco_to_yolo/scripts/convert_taco_dataset_to_yolo_format.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The purpose of this notebook is to convert a COCO-ish dataset to a format that can be used to train an Ultralytics YOLO model.\n",
        "\n",
        "# Using this notebook: workflow\n",
        "\n",
        "To use, make a copy of this notebook, and adapt it to work with your specific dataset. Please save your version of this ipynb file on GitHub in *recyclo/scripts*.\n",
        "\n",
        "(File > Save a copy in GitHub > File path = \"scripts/my_filename.ipynb\" to save notebook in scripts folder)\n",
        "\n",
        "Once you've generated your YOLO dataset, and are confident you can train a model with it, please upload your converted dataset to the Recyclo datasets google drive, https://drive.google.com/drive/folders/1bUkIYQRXX08OKI5TuOSg-eqntSudGaFB.\n",
        "\n",
        "(Why Google Drive? Because these datasets are too large for GitHub!)\n",
        "\n",
        "# What's in this notebook: contents\n",
        "\n",
        "Notebook contents:\n",
        "- intro to YOLO\n",
        "- intro to COCO\n",
        "- dataset specific notes (update for your specific dataset)\n",
        "- convert dataset to generic COCO (update for your specific dataset)\n",
        "- convert generic COCO to YOLO\n",
        "\n",
        "# Pro tips about Colabs\n",
        "\n",
        "When you open the \"Files\" tab on the left, you'll find yourself in a folder containing\n",
        "* ..\n",
        "* sample data\n",
        "\n",
        "This is a colab thing, the \"content\" folder, to get you started.\n",
        "Ignore it: click the .. to go up a level."
      ],
      "metadata": {
        "id": "WYhAfcih9-9h"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Intro to YOLO\n",
        "\n",
        "## General\n",
        "\n",
        "In general, YOLO models output the following for a given image:\n",
        "* Bounding box\n",
        "* Class label\n",
        "* Confidence score\n",
        "\n",
        "To train a YOLO model, we need object detection datasets that contain images of what we're looking for (trash), and annotations: class labels and bounding boxes.\n",
        "\n",
        "## Ultralytics YOLO\n",
        "\n",
        "In this project we will use Ultralytics YOLO object detection, eg their YOLO11n model. YOLO11n is a pretrained object detection model developed by Ultralytics.\n",
        "\n",
        "YOLO expects datasets in the following format:\n",
        "\n",
        "```\n",
        "dataset/\n",
        "├── images/\n",
        "│   ├── train/  <-- image files for training.\n",
        "│   ├── val/    <-- image files for validation after each epoch. Must not overlap with images in train.\n",
        "|   └── test/   <-- optional: can put some image files here for benchmarking.\n",
        "├── labels/\n",
        "│   ├── train/  <-- one .txt file per train image (must have same name). Contains class and bbox info..\n",
        "│   ├── val/    <-- one .txt file per val image.\n",
        "|   └── test/   <-- one .txt file per test image.\n",
        "└── data.yaml   <-- config file; helps tie all the above together.\n",
        "```\n",
        "\n",
        "Example labels/train file:\n",
        "```\n",
        "<class_id> <x_center> <y_center> <width> <height>\n",
        "```\n",
        "\n",
        "Example data.yaml file:\n",
        "```\n",
        "path: /content/dataset  # Root folder\n",
        "train: images/train\n",
        "val: images/val\n",
        "\n",
        "nc: 5  # number of classes\n",
        "names: ['bottle', 'can', 'plastic bag', 'wrapper', 'paper']\n",
        "\n",
        "```"
      ],
      "metadata": {
        "id": "x347PAo8rkYn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Intro to COCO\n",
        "\n",
        "## General\n",
        "COCO, Common Objects in Context, is a object detection, segmentation, and captioning dataset developed by Microsoft. It uses an annotations.json file to organize image data. This json annotation approach has become standard for other datasets to use.\n",
        "\n",
        "Lots of datasets use COCO-style of formatting. In addition to the training images themselves, these datasets have at least one annotations json file which contains the following:\n",
        "*   \"images\":  List of image metadata\n",
        "*   \"annotations\":  List of label data (type of trash, bounding box definition, segmentation data; corresponds to images list)\n",
        "*   \"categories\":  List of the different categories this dataset uses\n",
        "\n",
        "An example of a COCO-style dataset file structure is as follows:\n",
        "```\n",
        "dataset/\n",
        "├── annotations/\n",
        "│   ├── instances_train2017.json\n",
        "│   ├── instances_val2017.json\n",
        "│   ├── person_keypoints_train2017.json\n",
        "│   ├── captions_train2017.json\n",
        "│   └── ... (other task-specific .json files)\n",
        "├── images/\n",
        "│   ├── train2017/\n",
        "│   │   ├── 000000000009.jpg\n",
        "│   │   ├── 000000000025.jpg\n",
        "│   │   └── ...\n",
        "│   └── val2017/\n",
        "│       ├── 000000000139.jpg\n",
        "│       ├── 000000000285.jpg\n",
        "│       └── ...\n",
        "└── LICENSE.txt (optional)\n",
        "```\n",
        "\n",
        "## COCO format required by the COCO-to-YOLO conversion function\n",
        "\n",
        "\n",
        "To use the COCO to YOLO conversion function below, your data set must conform to the following (vastly simplified) COCO-like directory structure and json structure. It's unlikely that your dataset will conform to these specifications out of the box, so please use the code section below to modify your data's structure to match.\n",
        "\n",
        "The COCO directory structure must be as follows, with a folder called \"dataset\" located in your \"content\" directory:\n",
        "```\n",
        "dataset/\n",
        "├── images/\n",
        "│   ├── 000001.jpg  # or png or whatever\n",
        "│   ├── 000002.jpg\n",
        "│   └── ...\n",
        "└── annotations.json\n",
        "```\n",
        "\n",
        "And the annotations.json file must contain information in the following structure, and using the following json keywords:\n",
        "```\n",
        "{\n",
        "  \"images\": [\n",
        "    {\n",
        "      \"id\": 1,\n",
        "      \"file_name\": \"000001.jpg\",\n",
        "      \"width\": 640,\n",
        "      \"height\": 480\n",
        "    },\n",
        "    {\n",
        "      \"id\": 2,\n",
        "      \"file_name\": \"000002.jpg\",\n",
        "      \"width\": 800,\n",
        "      \"height\": 600\n",
        "    }\n",
        "  ],\n",
        "  \"annotations\": [\n",
        "    {\n",
        "      \"id\": 1,\n",
        "      \"image_id\": 1,\n",
        "      \"category_id\": 1,\n",
        "      \"bbox\": [100, 120, 50, 60],\n",
        "      \"area\": 3000,\n",
        "      \"iscrowd\": 0\n",
        "    },\n",
        "    {\n",
        "      \"id\": 2,\n",
        "      \"image_id\": 2,\n",
        "      \"category_id\": 2,\n",
        "      \"bbox\": [20, 30, 40, 50],\n",
        "      \"area\": 2000,\n",
        "      \"iscrowd\": 0\n",
        "    }\n",
        "  ],\n",
        "  \"categories\": [\n",
        "    {\n",
        "      \"id\": 1,\n",
        "      \"name\": \"plastic\"\n",
        "    },\n",
        "    {\n",
        "      \"id\": 2,\n",
        "      \"name\": \"metal\"\n",
        "    }\n",
        "  ]\n",
        "}\n",
        "```\n"
      ],
      "metadata": {
        "id": "wKqkU54arQMg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Helpful functions\n",
        "\n",
        "This section has some helpful functions you can use later in this notebook."
      ],
      "metadata": {
        "id": "epBrA9Nccr7u"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "from pprint import pprint\n",
        "\n",
        "def show_first_two_per_category(json_path):\n",
        "    \"\"\"\n",
        "    Prints the first two entries of each root-level list in a JSON file.\n",
        "\n",
        "    Useful for quickly inspecting the structure and content of an COCO-style\n",
        "    annotations json file.\n",
        "\n",
        "    It pretty-prints the first two entries of each top-level key that contains a list.\n",
        "\n",
        "    Args:\n",
        "        json_path (str or Path): Path to the JSON file to inspect.\n",
        "\n",
        "    Raises:\n",
        "        FileNotFoundError: If the provided path does not point to an existing file.\n",
        "        json.JSONDecodeError: If the file is not valid JSON.\n",
        "    \"\"\"\n",
        "    json_path = Path(json_path)\n",
        "\n",
        "    if not json_path.exists():\n",
        "        print(f\"File not found: {json_path}\")\n",
        "        return\n",
        "\n",
        "    with open(json_path, 'r') as f:\n",
        "        data = json.load(f)\n",
        "\n",
        "    for key, value in data.items():\n",
        "        print(f\"\\n--- {key.upper()} (showing first 2 entries) ---\")\n",
        "        if isinstance(value, list):\n",
        "            for item in value[:2]:\n",
        "                pprint(item)\n",
        "        else:\n",
        "            print(f\"{key} is not a list, skipping.\")"
      ],
      "metadata": {
        "id": "CgZ9lFw7bpLv"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "---\n",
        "⚠️ ***CHANGE THIS FILE FROM HERE DOWN TO SUIT YOUR DATASET*** ⚠️\n",
        "\n",
        "The sections above apply for all dataset conversions.\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "GUP0h7VX_seR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Sometimes this has to be run twice for the data to show up in the file tree\n",
        "import kagglehub\n",
        "from pathlib import Path\n",
        "\n",
        "taco_dataset_path = Path(kagglehub.dataset_download('kneroma/tacotrashdataset'))  # https://www.kaggle.com/datasets/kneroma/tacotrashdataset\n",
        "print(f\"Dataset downloaded to {taco_dataset_path}\\n\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DwT18ruVap0W",
        "outputId": "958ce8cf-665b-4a57-8a41-815fc5c555c8"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset downloaded to /kaggle/input/tacotrashdataset\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# If your dataset has one or more annotations json files, inspect them to help discover some of the quirks your dataset has.\n",
        "show_first_two_per_category(taco_dataset_path / \"data\" / \"annotations.json\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r4jKmumvccnN",
        "outputId": "7bd37408-7d21-4e17-9853-b715813ba77d"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- INFO (showing first 2 entries) ---\n",
            "info is not a list, skipping.\n",
            "\n",
            "--- IMAGES (showing first 2 entries) ---\n",
            "{'coco_url': None,\n",
            " 'date_captured': None,\n",
            " 'file_name': 'batch_1/000006.jpg',\n",
            " 'flickr_640_url': 'https://farm66.staticflickr.com/65535/33978196618_632623b4fc_z.jpg',\n",
            " 'flickr_url': 'https://farm66.staticflickr.com/65535/33978196618_e30a59e0a8_o.png',\n",
            " 'height': 2049,\n",
            " 'id': 0,\n",
            " 'license': None,\n",
            " 'width': 1537}\n",
            "{'coco_url': None,\n",
            " 'date_captured': None,\n",
            " 'file_name': 'batch_1/000008.jpg',\n",
            " 'flickr_640_url': 'https://farm66.staticflickr.com/65535/47803331152_19beae025a_z.jpg',\n",
            " 'flickr_url': 'https://farm66.staticflickr.com/65535/47803331152_ee00755a2e_o.png',\n",
            " 'height': 2049,\n",
            " 'id': 1,\n",
            " 'license': None,\n",
            " 'width': 1537}\n",
            "\n",
            "--- ANNOTATIONS (showing first 2 entries) ---\n",
            "{'area': 403954.0,\n",
            " 'bbox': [517.0, 127.0, 447.0, 1322.0],\n",
            " 'category_id': 6,\n",
            " 'id': 1,\n",
            " 'image_id': 0,\n",
            " 'iscrowd': 0,\n",
            " 'segmentation': [[561.0,\n",
            "                   1238.0,\n",
            "                   568.0,\n",
            "                   1201.0,\n",
            "                   567.0,\n",
            "                   1175.0,\n",
            "                   549.0,\n",
            "                   1127.0,\n",
            "                   538.0,\n",
            "                   1089.0,\n",
            "                   519.0,\n",
            "                   1043.0,\n",
            "                   517.0,\n",
            "                   1005.0,\n",
            "                   523.0,\n",
            "                   964.0,\n",
            "                   529.0,\n",
            "                   945.0,\n",
            "                   520.0,\n",
            "                   896.0,\n",
            "                   525.0,\n",
            "                   862.0,\n",
            "                   536.0,\n",
            "                   821.0,\n",
            "                   554.0,\n",
            "                   769.0,\n",
            "                   577.0,\n",
            "                   727.0,\n",
            "                   595.0,\n",
            "                   678.0,\n",
            "                   596.0,\n",
            "                   585.0,\n",
            "                   588.0,\n",
            "                   346.0,\n",
            "                   581.0,\n",
            "                   328.0,\n",
            "                   569.0,\n",
            "                   306.0,\n",
            "                   570.0,\n",
            "                   276.0,\n",
            "                   576.0,\n",
            "                   224.0,\n",
            "                   560.0,\n",
            "                   205.0,\n",
            "                   564.0,\n",
            "                   170.0,\n",
            "                   578.0,\n",
            "                   154.0,\n",
            "                   608.0,\n",
            "                   136.0,\n",
            "                   649.0,\n",
            "                   127.0,\n",
            "                   688.0,\n",
            "                   127.0,\n",
            "                   726.0,\n",
            "                   129.0,\n",
            "                   759.0,\n",
            "                   141.0,\n",
            "                   784.0,\n",
            "                   153.0,\n",
            "                   792.0,\n",
            "                   177.0,\n",
            "                   788.0,\n",
            "                   193.0,\n",
            "                   782.0,\n",
            "                   209.0,\n",
            "                   792.0,\n",
            "                   238.0,\n",
            "                   802.0,\n",
            "                   271.0,\n",
            "                   802.0,\n",
            "                   294.0,\n",
            "                   791.0,\n",
            "                   319.0,\n",
            "                   789.0,\n",
            "                   360.0,\n",
            "                   794.0,\n",
            "                   395.0,\n",
            "                   810.0,\n",
            "                   529.0,\n",
            "                   819.0,\n",
            "                   609.0,\n",
            "                   841.0,\n",
            "                   675.0,\n",
            "                   882.0,\n",
            "                   728.0,\n",
            "                   916.0,\n",
            "                   781.0,\n",
            "                   928.0,\n",
            "                   802.0,\n",
            "                   938.0,\n",
            "                   834.0,\n",
            "                   940.0,\n",
            "                   856.0,\n",
            "                   939.0,\n",
            "                   873.0,\n",
            "                   938.0,\n",
            "                   884.0,\n",
            "                   944.0,\n",
            "                   901.0,\n",
            "                   951.0,\n",
            "                   917.0,\n",
            "                   956.0,\n",
            "                   942.0,\n",
            "                   960.0,\n",
            "                   972.0,\n",
            "                   964.0,\n",
            "                   1013.0,\n",
            "                   959.0,\n",
            "                   1036.0,\n",
            "                   952.0,\n",
            "                   1081.0,\n",
            "                   952.0,\n",
            "                   1106.0,\n",
            "                   934.0,\n",
            "                   1163.0,\n",
            "                   935.0,\n",
            "                   1174.0,\n",
            "                   949.0,\n",
            "                   1209.0,\n",
            "                   954.0,\n",
            "                   1235.0,\n",
            "                   952.0,\n",
            "                   1273.0,\n",
            "                   953.0,\n",
            "                   1296.0,\n",
            "                   946.0,\n",
            "                   1320.0,\n",
            "                   930.0,\n",
            "                   1347.0,\n",
            "                   914.0,\n",
            "                   1367.0,\n",
            "                   898.0,\n",
            "                   1388.0,\n",
            "                   855.0,\n",
            "                   1419.0,\n",
            "                   821.0,\n",
            "                   1437.0,\n",
            "                   769.0,\n",
            "                   1449.0,\n",
            "                   728.0,\n",
            "                   1448.0,\n",
            "                   687.0,\n",
            "                   1439.0,\n",
            "                   645.0,\n",
            "                   1419.0,\n",
            "                   618.0,\n",
            "                   1397.0,\n",
            "                   582.0,\n",
            "                   1348.0,\n",
            "                   572.0,\n",
            "                   1314.0,\n",
            "                   564.0,\n",
            "                   1279.0,\n",
            "                   561.0,\n",
            "                   1253.0,\n",
            "                   561.0,\n",
            "                   1238.0]]}\n",
            "{'area': 1071259.5,\n",
            " 'bbox': [1.0, 457.0, 1429.0, 1519.0],\n",
            " 'category_id': 18,\n",
            " 'id': 2,\n",
            " 'image_id': 1,\n",
            " 'iscrowd': 0,\n",
            " 'segmentation': [[928.0,\n",
            "                   1876.0,\n",
            "                   938.0,\n",
            "                   1856.0,\n",
            "                   968.0,\n",
            "                   1826.0,\n",
            "                   990.0,\n",
            "                   1808.0,\n",
            "                   998.0,\n",
            "                   1790.0,\n",
            "                   1069.0,\n",
            "                   1727.0,\n",
            "                   1096.0,\n",
            "                   1702.0,\n",
            "                   1159.0,\n",
            "                   1644.0,\n",
            "                   1212.0,\n",
            "                   1588.0,\n",
            "                   1258.0,\n",
            "                   1540.0,\n",
            "                   1314.0,\n",
            "                   1482.0,\n",
            "                   1357.0,\n",
            "                   1444.0,\n",
            "                   1392.0,\n",
            "                   1416.0,\n",
            "                   1409.0,\n",
            "                   1393.0,\n",
            "                   1430.0,\n",
            "                   1369.0,\n",
            "                   1415.0,\n",
            "                   1347.0,\n",
            "                   1130.0,\n",
            "                   1087.0,\n",
            "                   780.0,\n",
            "                   763.0,\n",
            "                   528.0,\n",
            "                   533.0,\n",
            "                   479.0,\n",
            "                   486.0,\n",
            "                   466.0,\n",
            "                   466.0,\n",
            "                   448.0,\n",
            "                   457.0,\n",
            "                   427.0,\n",
            "                   468.0,\n",
            "                   387.0,\n",
            "                   502.0,\n",
            "                   321.0,\n",
            "                   554.0,\n",
            "                   244.0,\n",
            "                   608.0,\n",
            "                   118.0,\n",
            "                   693.0,\n",
            "                   37.0,\n",
            "                   750.0,\n",
            "                   3.0,\n",
            "                   780.0,\n",
            "                   1.0,\n",
            "                   995.0,\n",
            "                   28.0,\n",
            "                   1032.0,\n",
            "                   104.0,\n",
            "                   1119.0,\n",
            "                   403.0,\n",
            "                   1471.0,\n",
            "                   666.0,\n",
            "                   1805.0,\n",
            "                   763.0,\n",
            "                   1954.0,\n",
            "                   782.0,\n",
            "                   1945.0,\n",
            "                   796.0,\n",
            "                   1970.0,\n",
            "                   803.0,\n",
            "                   1976.0,\n",
            "                   818.0,\n",
            "                   1976.0,\n",
            "                   836.0,\n",
            "                   1956.0,\n",
            "                   852.0,\n",
            "                   1954.0,\n",
            "                   860.0,\n",
            "                   1937.0,\n",
            "                   873.0,\n",
            "                   1931.0,\n",
            "                   885.0,\n",
            "                   1908.0,\n",
            "                   898.0,\n",
            "                   1896.0,\n",
            "                   928.0,\n",
            "                   1876.0]]}\n",
            "\n",
            "--- SCENE_ANNOTATIONS (showing first 2 entries) ---\n",
            "{'background_ids': [1], 'image_id': 0}\n",
            "{'background_ids': [1], 'image_id': 1}\n",
            "\n",
            "--- LICENSES (showing first 2 entries) ---\n",
            "\n",
            "--- CATEGORIES (showing first 2 entries) ---\n",
            "{'id': 0, 'name': 'Aluminium foil', 'supercategory': 'Aluminium foil'}\n",
            "{'id': 1, 'name': 'Battery', 'supercategory': 'Battery'}\n",
            "\n",
            "--- SCENE_CATEGORIES (showing first 2 entries) ---\n",
            "{'id': 0, 'name': 'Clean'}\n",
            "{'id': 1, 'name': 'Indoor, Man-made'}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# TACO dataset\n",
        "\n",
        "The TACO dataset uses COCO-style formatting (segmentation).\n",
        "\n",
        "## Quirks\n",
        "\n",
        "TACO has 15 batch folders, each containing jpgs with names like 000000.jpg, 000001.jpg, 000002.jpg, etc.\n",
        "* Some numbers are skipped, eg 000000.jpg, 000001.jpg, 000003.jpg.\n",
        "* The name 000000.jpg is used in multiple batch folders, to name different image files.\n",
        "* It contains segmentation data (will be discarded).\n",
        "* It uses categories (specific) and supercategories (more general) - only the supercategories will be retained.\n",
        "* It includes scene categories (eg clearn streets) - this will be discarded.\n",
        "  * It might be interesting in the future to consider using a multi-tasking model - detecting trash and also classsifying the scene might improve the models performance by adding a bit of context awareness.\n",
        "\n",
        "## Conversion\n",
        "To convert the TACO dataset to a format ultralytics YOLO can use, we must:\n",
        "* Give the images unique names\n",
        "* Split the TACO images into train and val sets\n",
        "* Extract label and bbox info from annotations.json, and save it in individual txt files corresponding to the image files\n",
        "* Make a data.yaml file"
      ],
      "metadata": {
        "id": "f7u6PWyfxeLD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Modify your dataset structure to match the required simple-COCO style\n",
        "import shutil\n",
        "\n",
        "source_root = taco_dataset_path / \"data\"\n",
        "annotations_path = source_root / 'annotations.json'\n",
        "\n",
        "target_root = Path('/content/dataset')\n",
        "target_img_dir = target_root / 'images'\n",
        "if target_root.exists():\n",
        "    shutil.rmtree(target_root)\n",
        "target_img_dir.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "with annotations_path.open('r') as f:\n",
        "    annotations = json.load(f)\n",
        "\n",
        "# Reassign file names with unique zero-padded numbering\n",
        "image_id_map = {}\n",
        "for idx, image_info in enumerate(annotations['images']):\n",
        "    old_path = source_root / image_info['file_name']\n",
        "    new_filename = f\"{idx:06}.jpg\"\n",
        "    new_path = target_img_dir / new_filename\n",
        "\n",
        "    image_id_map[old_path] = new_path\n",
        "    image_info['file_name'] = new_filename\n",
        "\n",
        "    # Keep only required image fields\n",
        "    annotations['images'][idx] = {\n",
        "        'id': image_info['id'],\n",
        "        'file_name': new_filename,\n",
        "        'width': image_info['width'],\n",
        "        'height': image_info['height']\n",
        "    }\n",
        "\n",
        "# Prune annotation fields\n",
        "for ann in annotations['annotations']:\n",
        "    ann_keys = ['id', 'image_id', 'category_id', 'bbox', 'area', 'iscrowd']\n",
        "    for key in list(ann.keys()):\n",
        "        if key not in ann_keys:\n",
        "            del ann[key]\n",
        "\n",
        "# Update category entries to only include id and name = supercategory\n",
        "for cat in annotations['categories']:\n",
        "    cat_id = cat['id']\n",
        "    cat_super = cat.get('supercategory', f'category_{cat_id}')\n",
        "    cat.clear()\n",
        "    cat['id'] = cat_id\n",
        "    cat['name'] = cat_super\n",
        "\n",
        "# Copy image files to new location\n",
        "for src, dst in image_id_map.items():\n",
        "    if src.exists():\n",
        "        shutil.copy(src, dst)\n",
        "    else:\n",
        "        print(f\"Warning: Missing image: {src}\")\n",
        "\n",
        "# Remove unwanted top-level keys\n",
        "for key in ['info', 'scene_annotations', 'scene_categories', 'licenses']:\n",
        "    annotations.pop(key, None)\n",
        "\n",
        "# Save cleaned annotations\n",
        "new_annotations_path = target_root / 'annotations.json'\n",
        "with new_annotations_path.open('w') as f:\n",
        "    json.dump(annotations, f)\n",
        "\n",
        "print(f\"Converted and cleaned {len(image_id_map)} images.\")"
      ],
      "metadata": {
        "id": "Usb6nID5YC8i",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "06f7fc44-3036-430e-8dc5-5ddb74756f28"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Converted and cleaned 1500 images.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Inspect your new annotations.json file to make sure the format and field names match the simple COCO format described above\n",
        "show_first_two_per_category(target_root / \"annotations.json\")"
      ],
      "metadata": {
        "id": "EB3t3Ey-hq9E",
        "outputId": "c1e24dfb-3ca5-4327-e9dd-ab77a67dfc6f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- IMAGES (showing first 2 entries) ---\n",
            "{'file_name': '000000.jpg', 'height': 2049, 'id': 0, 'width': 1537}\n",
            "{'file_name': '000001.jpg', 'height': 2049, 'id': 1, 'width': 1537}\n",
            "\n",
            "--- ANNOTATIONS (showing first 2 entries) ---\n",
            "{'area': 403954.0,\n",
            " 'bbox': [517.0, 127.0, 447.0, 1322.0],\n",
            " 'category_id': 6,\n",
            " 'id': 1,\n",
            " 'image_id': 0,\n",
            " 'iscrowd': 0}\n",
            "{'area': 1071259.5,\n",
            " 'bbox': [1.0, 457.0, 1429.0, 1519.0],\n",
            " 'category_id': 18,\n",
            " 'id': 2,\n",
            " 'image_id': 1,\n",
            " 'iscrowd': 0}\n",
            "\n",
            "--- CATEGORIES (showing first 2 entries) ---\n",
            "{'id': 0, 'name': 'Aluminium foil'}\n",
            "{'id': 1, 'name': 'Battery'}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "import shutil\n",
        "import random\n",
        "from pathlib import Path\n",
        "from sklearn.model_selection import train_test_split\n",
        "import yaml\n",
        "\n",
        "# === CONFIG ===\n",
        "input_root = Path(taco_dataset_path)\n",
        "output_root = Path('/kaggle/working/taco_yolo')   # Since TACO is a kaggle dataset, this will output to kaggle/working/taco_yolo\n",
        "train_ratio = 0.8  # 80% training, 20% validation\n",
        "\n",
        "# === OUTPUT STRUCTURE ===\n",
        "images_train = output_root / 'images' / 'train'\n",
        "images_val = output_root / 'images' / 'val'\n",
        "labels_train = output_root / 'labels' / 'train'\n",
        "labels_val = output_root / 'labels' / 'val'\n",
        "\n",
        "# Create folders\n",
        "for folder in [images_train, images_val, labels_train, labels_val]:\n",
        "    folder.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "# === LOAD ANNOTATIONS ===\n",
        "with open(input_root / 'data' / 'annotations.json', 'r') as f:\n",
        "    coco = json.load(f)\n",
        "\n",
        "image_id_to_info = {img['id']: img for img in coco['images']}\n",
        "annotations_by_image = {}\n",
        "\n",
        "for ann in coco['annotations']:\n",
        "    image_id = ann['image_id']\n",
        "    annotations_by_image.setdefault(image_id, []).append(ann)\n",
        "\n",
        "category_map = {cat['id']: idx for idx, cat in enumerate(coco['categories'])}\n",
        "category_names = [cat['name'] for cat in sorted(coco['categories'], key=lambda x: category_map[x['id']])]\n",
        "\n",
        "# === SPLIT DATA ===\n",
        "all_image_ids = list(image_id_to_info.keys())\n",
        "train_ids, val_ids = train_test_split(all_image_ids, train_size=train_ratio, random_state=42)\n",
        "\n",
        "def convert_bbox_to_yolo(bbox, img_w, img_h):\n",
        "    \"\"\"Convert COCO bbox to normalized YOLO format.\n",
        "\n",
        "    Args:\n",
        "        bbox (tuple): Bounding box as (x, y, w, h).\n",
        "        img_w (int): Image width.\n",
        "        img_h (int): Image height.\n",
        "\n",
        "    Returns:\n",
        "        tuple: Normalized (x_center, y_center, w, h).\n",
        "    \"\"\"\n",
        "    x, y, w, h = bbox\n",
        "    x_center = (x + w / 2) / img_w\n",
        "    y_center = (y + h / 2) / img_h\n",
        "    w /= img_w\n",
        "    h /= img_h\n",
        "    return x_center, y_center, w, h\n",
        "\n",
        "def process_image(image_id, split):\n",
        "    \"\"\"Processes a single image and its annotations for YOLO training.\n",
        "\n",
        "    Args:\n",
        "        image_id (str): Unique identifier for the image.\n",
        "        split (str): Either 'train' or 'val', indicating dataset split.\n",
        "\n",
        "    Returns:\n",
        "        None. Writes output image and label files to file system.\n",
        "\n",
        "    \"\"\"\n",
        "    image_info = image_id_to_info[image_id]\n",
        "    img_path = input_root / 'data' / image_info['file_name']\n",
        "    img_w, img_h = image_info['width'], image_info['height']\n",
        "\n",
        "    # Output paths\n",
        "    out_img_dir = images_train if split == 'train' else images_val\n",
        "    out_label_dir = labels_train if split == 'train' else labels_val\n",
        "\n",
        "    # The TACO dataset has multiple different batches, which contain jpgs\n",
        "    # that have a 6 digit name. Different pictures sometimes have the same name,\n",
        "    # eg 000123.jpg in the batch_1 folder might be a different image than\n",
        "    # 000123.jpg in batch_10. To ensure unique names for all images, append\n",
        "    # the batch name to the image, so \"000123.jpg\" in the batch_1 folder\n",
        "    # becomes \"batch_1_000123.jpg\".\n",
        "    rel_path = Path(image_info['file_name'])  # e.g., batch_1/000123.jpg\n",
        "    batch_name = rel_path.parts[0]\n",
        "    batch_prefix = batch_name.replace('/', '_')\n",
        "    filename = f\"{batch_prefix}_{rel_path.name}\"\n",
        "\n",
        "    # Output flattened path\n",
        "    out_img_path = out_img_dir / filename\n",
        "    out_label_path = out_label_dir / filename.replace('.jpg', '.txt')\n",
        "\n",
        "    # Copy image\n",
        "    shutil.copy(input_root / 'data' / rel_path, out_img_path)\n",
        "\n",
        "    # Write label\n",
        "    with open(out_label_path, 'w') as label_file:\n",
        "        for ann in annotations_by_image.get(image_id, []):\n",
        "            class_id = category_map[ann['category_id']]\n",
        "            bbox = convert_bbox_to_yolo(ann['bbox'], img_w, img_h)\n",
        "            label_file.write(f\"{class_id} {' '.join(f'{x:.6f}' for x in bbox)}\\n\")\n",
        "\n",
        "# Process images\n",
        "for image_id in train_ids:\n",
        "    process_image(image_id, 'train')\n",
        "\n",
        "for image_id in val_ids:\n",
        "    process_image(image_id, 'val')\n",
        "\n",
        "# === WRITE data.yaml ===\n",
        "data_yaml = {\n",
        "    'path': str(output_root),\n",
        "    'train': 'images/train',\n",
        "    'val': 'images/val',\n",
        "    'nc': len(category_names),\n",
        "    'names': category_names\n",
        "}\n",
        "\n",
        "with open(output_root / 'data.yaml', 'w') as f:\n",
        "    yaml.dump(data_yaml, f)\n",
        "\n",
        "print(\"Conversion complete. YOLO dataset created at:\", output_root)"
      ],
      "metadata": {
        "id": "aphWONrIrj8y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "To verify that your conversion worked, make sure you can train a model and that it outputs images with a bounding box and label."
      ],
      "metadata": {
        "id": "xpsMcME2-rDs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -U ultralytics\n",
        "\n",
        "from ultralytics import YOLO\n",
        "import os"
      ],
      "metadata": {
        "collapsed": true,
        "id": "Mb5g0TegvNCl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = YOLO('yolo11n.pt')\n",
        "results = model.train(data='/kaggle/working/taco_yolo/data.yaml', epochs=5, imgsz=640)  # epoch size is small - this is just to see if it can work!"
      ],
      "metadata": {
        "collapsed": true,
        "id": "UL4rIlXU-oLt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "If the model outputs even one image with a bounding box and label, then the dataset should work for our project! Verify this using the code below."
      ],
      "metadata": {
        "id": "2JPNfPNAEtFK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import cv2\n",
        "from random import sample\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "model = YOLO('runs/detect/train/weights/best.pt')\n",
        "\n",
        "train_images_path = Path('/kaggle/working/taco_yolo/images/train')\n",
        "image_files = list(train_images_path.glob('*.jpg'))\n",
        "\n",
        "sample_images = sample(image_files, 10)\n",
        "\n",
        "for image_path in sample_images:\n",
        "    result = model(image_path)[0]\n",
        "    annotated_image = result.plot()\n",
        "\n",
        "    plt.figure(figsize=(8, 6))\n",
        "    plt.imshow(annotated_image)\n",
        "    plt.title(f'Predictions: {image_path.name}')\n",
        "    plt.axis('off')\n",
        "    plt.show()"
      ],
      "metadata": {
        "collapsed": true,
        "id": "Apy8pfX6Eyhe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "If the model successfully generated even one image with a bounding box and label, please run the following code block to zip the taco_yolo dataset, download the zipped file, and upload it on Google Drive, https://www.google.com/url?q=https%3A%2F%2Fdrive.google.com%2Fdrive%2Ffolders%2F1bUkIYQRXX08OKI5TuOSg-eqntSudGaFB."
      ],
      "metadata": {
        "id": "zj5c-mtlL2gk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from datetime import datetime\n",
        "\n",
        "dataset_name = 'taco_yolo'\n",
        "\n",
        "# Generate date prefix\n",
        "date_str = datetime.now().strftime('%Y%m%d')\n",
        "zip_name = f\"{date_str}_{dataset_name}.zip\"\n",
        "\n",
        "# Change directory and zip\n",
        "%cd /kaggle/working/{dataset_name}\n",
        "!zip -r /content/{zip_name} .\n",
        "\n",
        "print(f\"Zip created at /content/{zip_name}\")"
      ],
      "metadata": {
        "collapsed": true,
        "id": "Ik8EKrBMLyap"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}