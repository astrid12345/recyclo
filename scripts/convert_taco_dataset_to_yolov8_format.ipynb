{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNbRd3alRGC/9rBe4RGPGQs",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/astrid12345/recyclo/blob/convert_taco_to_yolo/scripts/convert_taco_dataset_to_yolov8_format.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The purpose of this notebook is to convert a COCO-ish dataset to a format that can be used to train a YOLOv8 model.\n",
        "\n",
        "# Using this notebook: workflow\n",
        "\n",
        "To use, make a copy of this notebook, and adapt it to work with your specific dataset. Please save your version of this ipynb file on GitHub in *recyclo/scripts*.\n",
        "\n",
        "(File > Save a copy in GitHub > File path = \"scripts/my_filename.ipynb\" to save notebook in scripts folder)\n",
        "\n",
        "Once you've generated your YOLOv8 dataset, and are confident you can train a model with it, please upload your converted dataset to the Recyclo datasets google drive, https://drive.google.com/drive/folders/1bUkIYQRXX08OKI5TuOSg-eqntSudGaFB.\n",
        "\n",
        "(Why Google Drive? Because these datasets are too large for GitHub!)\n",
        "\n",
        "# What's in this notebook: contents\n",
        "\n",
        "Notebook contents:\n",
        "- intro to YOLO\n",
        "- intro to COCO\n",
        "- dataset specific notes (update for your specific dataset)\n",
        "- convert dataset to generic COCO (update for your specific dataset)\n",
        "- convert generic COCO to YOLO\n",
        "\n",
        "# Pro tips about Colabs\n",
        "\n",
        "When you open the \"Files\" tab on the left, you'll find yourself in a folder containing\n",
        "* ..\n",
        "* sample data\n",
        "\n",
        "This is a colab thing, the \"content\" folder, to get you started.\n",
        "Ignore it: click the .. to go up a level."
      ],
      "metadata": {
        "id": "WYhAfcih9-9h"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Intro to YOLO\n",
        "\n",
        "## General\n",
        "\n",
        "In general, YOLO models output the following for a given image:\n",
        "* Bounding box\n",
        "* Class label\n",
        "* Confidence score\n",
        "\n",
        "To train a YOLO model, we need object detection datasets that contain images of what we're looking for (trash), and annotations: class labels and bounding boxes.\n",
        "\n",
        "## Ultralytics YOLO\n",
        "\n",
        "In this project we will use Ultralytics YOLO object detection, eg their YOLOv8 model, or YOLO11n. YOLOv8 is a pretrain object detection model developed by Ultralytics.\n",
        "\n",
        "YOLO expects datasets in the following format:\n",
        "\n",
        "```\n",
        "dataset/\n",
        "├── images/\n",
        "│   ├── train/  <-- image files for training\n",
        "│   └── val/    <-- image files for validation after each epoch. Must not overlap with images in train.\n",
        "├── labels/\n",
        "│   ├── train/  <-- one .txt file per train image. Contains class and bbox info..\n",
        "│   └── val/    <-- one .txt file per val image\n",
        "└── data.yaml   <-- config file; helps tie all the above together.\n",
        "```\n",
        "\n",
        "Example labels/train file:\n",
        "```\n",
        "<class_id> <x_center> <y_center> <width> <height>\n",
        "```\n",
        "\n",
        "Example data.yaml file:\n",
        "```\n",
        "path: /content/dataset  # Root folder\n",
        "train: images/train\n",
        "val: images/val\n",
        "\n",
        "nc: 5  # number of classes\n",
        "names: ['bottle', 'can', 'plastic bag', 'wrapper', 'paper']\n",
        "\n",
        "```"
      ],
      "metadata": {
        "id": "x347PAo8rkYn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Intro to COCO\n",
        "\n",
        "## General\n",
        "COCO, Common Objects in Context, is a object detection, segmentation, and captioning dataset developed by Microsoft. It uses an annotations.json file to organize image data. This json annotation approach has become standard for other datasets to use.\n",
        "\n",
        "Lots of datasets use a COCO-style of formatting. In addition to the training images themselves, these datasets have an an annotations.json file which contains the following:\n",
        "*   \"images\":  List of image metadata\n",
        "*   \"annotations\":  List of label data (type of trash, bounding box definition, segmentation data; corresponds to images list)\n",
        "*   \"categories\":  List of the different categories this dataset uses\n",
        "\n",
        "Your dataset may have some quirks or unique ways in which it applies the COCO style; if so use the section below to clean up your dataset to a generic COCO style."
      ],
      "metadata": {
        "id": "wKqkU54arQMg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "---\n",
        "⚠️ ***CHANGE THIS FILE FROM HERE DOWN TO SUIT YOUR DATASET*** ⚠️\n",
        "\n",
        "The sections above apply for all dataset conversions.\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "GUP0h7VX_seR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# TACO dataset\n",
        "\n",
        "The TACO dataset uses COCO-style formatting (segmentation).\n",
        "\n",
        "## Quirks\n",
        "\n",
        "TACO has 15 batch folders, each containing jpgs with names like 000000.jpg, 000001.jpg, 000002.jpg, etc.\n",
        "* Some numbers are skipped, eg 000000.jpg, 000001.jpg, 000003.jpg\n",
        "* The name 000000.jpg is used in multiple batch folders, to name different image files.\n",
        "\n",
        "## Conversion\n",
        "To convert the TACO dataset to a format YOLOv8 can use, we must:\n",
        "* Give the images unique names\n",
        "* Split the TACO images into train and val sets\n",
        "* Extract label and bbox info from annotations.json, and save it in individual txt files corresponding to the image files\n",
        "* Make a data.yaml file"
      ],
      "metadata": {
        "id": "f7u6PWyfxeLD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "import shutil\n",
        "import random\n",
        "from pathlib import Path\n",
        "from sklearn.model_selection import train_test_split\n",
        "import yaml\n",
        "\n",
        "# === IMPORT DATASET ===\n",
        "import kagglehub\n",
        "taco_dataset_path = kagglehub.dataset_download('kneroma/tacotrashdataset')  # https://www.kaggle.com/datasets/kneroma/tacotrashdataset\n",
        "print(f\"Dataset downloaded to {taco_dataset_path}\\n\")\n",
        "\n",
        "# === CONFIG ===\n",
        "input_root = Path(taco_dataset_path)\n",
        "output_root = Path('/kaggle/working/taco_yolo')   # Since TACO is a kaggle dataset, this will output to kaggle/working/taco_yolo\n",
        "train_ratio = 0.8  # 80% training, 20% validation\n",
        "\n",
        "# === OUTPUT STRUCTURE ===\n",
        "images_train = output_root / 'images' / 'train'\n",
        "images_val = output_root / 'images' / 'val'\n",
        "labels_train = output_root / 'labels' / 'train'\n",
        "labels_val = output_root / 'labels' / 'val'\n",
        "\n",
        "# Create folders\n",
        "for folder in [images_train, images_val, labels_train, labels_val]:\n",
        "    folder.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "# === LOAD ANNOTATIONS ===\n",
        "with open(input_root / 'data' / 'annotations.json', 'r') as f:\n",
        "    coco = json.load(f)\n",
        "\n",
        "image_id_to_info = {img['id']: img for img in coco['images']}\n",
        "annotations_by_image = {}\n",
        "\n",
        "for ann in coco['annotations']:\n",
        "    image_id = ann['image_id']\n",
        "    annotations_by_image.setdefault(image_id, []).append(ann)\n",
        "\n",
        "category_map = {cat['id']: idx for idx, cat in enumerate(coco['categories'])}\n",
        "category_names = [cat['name'] for cat in sorted(coco['categories'], key=lambda x: category_map[x['id']])]\n",
        "\n",
        "# === SPLIT DATA ===\n",
        "all_image_ids = list(image_id_to_info.keys())\n",
        "train_ids, val_ids = train_test_split(all_image_ids, train_size=train_ratio, random_state=42)\n",
        "\n",
        "def convert_bbox_to_yolo(bbox, img_w, img_h):\n",
        "    \"\"\"Convert COCO bbox to normalized YOLO format.\n",
        "\n",
        "    Args:\n",
        "        bbox (tuple): Bounding box as (x, y, w, h).\n",
        "        img_w (int): Image width.\n",
        "        img_h (int): Image height.\n",
        "\n",
        "    Returns:\n",
        "        tuple: Normalized (x_center, y_center, w, h).\n",
        "    \"\"\"\n",
        "    x, y, w, h = bbox\n",
        "    x_center = (x + w / 2) / img_w\n",
        "    y_center = (y + h / 2) / img_h\n",
        "    w /= img_w\n",
        "    h /= img_h\n",
        "    return x_center, y_center, w, h\n",
        "\n",
        "def process_image(image_id, split):\n",
        "    \"\"\"Processes a single image and its annotations for YOLOv8 training.\n",
        "\n",
        "    Args:\n",
        "        image_id (str): Unique identifier for the image.\n",
        "        split (str): Either 'train' or 'val', indicating dataset split.\n",
        "\n",
        "    Returns:\n",
        "        None. Writes output image and label files to file system.\n",
        "\n",
        "    \"\"\"\n",
        "    image_info = image_id_to_info[image_id]\n",
        "    img_path = input_root / 'data' / image_info['file_name']\n",
        "    img_w, img_h = image_info['width'], image_info['height']\n",
        "\n",
        "    # Output paths\n",
        "    out_img_dir = images_train if split == 'train' else images_val\n",
        "    out_label_dir = labels_train if split == 'train' else labels_val\n",
        "\n",
        "    # The TACO dataset has multiple different batches, which contain jpgs\n",
        "    # that have a 6 digit name. Different pictures sometimes have the same name,\n",
        "    # eg 000123.jpg in the batch_1 folder might be a different image than\n",
        "    # 000123.jpg in batch_10. To ensure unique names for all images, append\n",
        "    # the batch name to the image, so \"000123.jpg\" in the batch_1 folder\n",
        "    # becomes \"batch_1_000123.jpg\".\n",
        "    rel_path = Path(image_info['file_name'])  # e.g., batch_1/000123.jpg\n",
        "    batch_name = rel_path.parts[0]\n",
        "    batch_prefix = batch_name.replace('/', '_')\n",
        "    filename = f\"{batch_prefix}_{rel_path.name}\"\n",
        "\n",
        "    # Output flattened path\n",
        "    out_img_path = out_img_dir / filename\n",
        "    out_label_path = out_label_dir / filename.replace('.jpg', '.txt')\n",
        "\n",
        "    # Copy image\n",
        "    shutil.copy(input_root / 'data' / rel_path, out_img_path)\n",
        "\n",
        "    # Write label\n",
        "    with open(out_label_path, 'w') as label_file:\n",
        "        for ann in annotations_by_image.get(image_id, []):\n",
        "            class_id = category_map[ann['category_id']]\n",
        "            bbox = convert_bbox_to_yolo(ann['bbox'], img_w, img_h)\n",
        "            label_file.write(f\"{class_id} {' '.join(f'{x:.6f}' for x in bbox)}\\n\")\n",
        "\n",
        "# Process images\n",
        "for image_id in train_ids:\n",
        "    process_image(image_id, 'train')\n",
        "\n",
        "for image_id in val_ids:\n",
        "    process_image(image_id, 'val')\n",
        "\n",
        "# === WRITE data.yaml ===\n",
        "data_yaml = {\n",
        "    'path': str(output_root),\n",
        "    'train': 'images/train',\n",
        "    'val': 'images/val',\n",
        "    'nc': len(category_names),\n",
        "    'names': category_names\n",
        "}\n",
        "\n",
        "with open(output_root / 'data.yaml', 'w') as f:\n",
        "    yaml.dump(data_yaml, f)\n",
        "\n",
        "print(\"Conversion complete. YOLOv8 dataset created at:\", output_root)"
      ],
      "metadata": {
        "id": "aphWONrIrj8y",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "eb2d9512-5abb-4609-82b5-9339328c9fb7"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading from https://www.kaggle.com/api/v1/datasets/download/kneroma/tacotrashdataset?dataset_version_number=3...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 2.79G/2.79G [00:20<00:00, 148MB/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting files...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset downloaded to /root/.cache/kagglehub/datasets/kneroma/tacotrashdataset/versions/3\n",
            "\n",
            "Conversion complete. YOLOv8 dataset created at: /kaggle/working/taco_yolo\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "To verify that your conversion worked, make sure you can train a model and that it outputs images with a bounding box and label."
      ],
      "metadata": {
        "id": "xpsMcME2-rDs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -U ultralytics\n",
        "\n",
        "from ultralytics import YOLO\n",
        "import os"
      ],
      "metadata": {
        "collapsed": true,
        "id": "Mb5g0TegvNCl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = YOLO('yolo8n.pt')\n",
        "results = model.train(data='/kaggle/working/taco_yolo/data.yaml', epochs=5, imgsz=640)  # epoch size is small - this is just to see if it can work!"
      ],
      "metadata": {
        "collapsed": true,
        "id": "UL4rIlXU-oLt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "If the model outputs even one image with a bounding box and label, then the dataset should work for our project! Verify this using the code below."
      ],
      "metadata": {
        "id": "2JPNfPNAEtFK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import cv2\n",
        "from random import sample\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "model = YOLO('runs/detect/train/weights/best.pt')\n",
        "\n",
        "train_images_path = Path('/kaggle/working/taco_yolo/images/train')\n",
        "image_files = list(train_images_path.glob('*.jpg'))\n",
        "\n",
        "sample_images = sample(image_files, 10)\n",
        "\n",
        "for image_path in sample_images:\n",
        "    result = model(image_path)[0]\n",
        "    annotated_image = result.plot()\n",
        "\n",
        "    plt.figure(figsize=(8, 6))\n",
        "    plt.imshow(annotated_image)\n",
        "    plt.title(f'Predictions: {image_path.name}')\n",
        "    plt.axis('off')\n",
        "    plt.show()"
      ],
      "metadata": {
        "collapsed": true,
        "id": "Apy8pfX6Eyhe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "If the model successfully generated even one image with a bounding box and label, please run the following code block to zip the taco_yolo dataset, download the zipped file, and upload it on Google Drive, https://www.google.com/url?q=https%3A%2F%2Fdrive.google.com%2Fdrive%2Ffolders%2F1bUkIYQRXX08OKI5TuOSg-eqntSudGaFB."
      ],
      "metadata": {
        "id": "zj5c-mtlL2gk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from datetime import datetime\n",
        "\n",
        "dataset_name = 'taco_yolo'\n",
        "\n",
        "# Generate date prefix\n",
        "date_str = datetime.now().strftime('%Y%m%d')\n",
        "zip_name = f\"{date_str}_{dataset_name}.zip\"\n",
        "\n",
        "# Change directory and zip\n",
        "%cd /kaggle/working/{dataset_name}\n",
        "!zip -r /content/{zip_name} .\n",
        "\n",
        "print(f\"Zip created at /content/{zip_name}\")"
      ],
      "metadata": {
        "collapsed": true,
        "id": "Ik8EKrBMLyap"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}