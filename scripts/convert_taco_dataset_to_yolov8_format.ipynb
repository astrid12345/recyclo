{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyME9zI1iXhTxecJXMI40Ee3",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/astrid12345/recyclo/blob/convert_taco_to_yolo/scripts/convert_taco_dataset_to_yolov8_format.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The purpose of this notebook is to convert a dataset to a format that can be used to train a YOLOv8 model.\n",
        "\n",
        "To use, make a copy of this notebook, and adapt it to work with your specific dataset. Please save your version of this ipynb file on GitHub in *recyclo/scripts*.\n",
        "\n",
        "(File > Save a copy in GitHub > File path = \"scripts/my_filename.ipynb\" to save notebook in scripts folder)\n",
        "\n",
        "Once you've generated your YOLOv8 dataset, and are confident you can train a model with it, please upload your converted dataset to the Recyclo datasets google drive, https://drive.google.com/drive/folders/1bUkIYQRXX08OKI5TuOSg-eqntSudGaFB.\n",
        "\n",
        "(Why Google Drive? Because these datasets are too large for GitHub!)"
      ],
      "metadata": {
        "id": "WYhAfcih9-9h"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -U ultralytics\n",
        "\n",
        "from ultralytics import YOLO\n",
        "import os"
      ],
      "metadata": {
        "collapsed": true,
        "id": "Mb5g0TegvNCl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Convert datasets for YOLOv8\n",
        "\n",
        "### General\n",
        "\n",
        "In general, YOLO models output the following for a given image:\n",
        "* Bounding box\n",
        "* Class label\n",
        "* Confidence score\n",
        "\n",
        "To train a YOLO model, we need object detection datasets that contain images of what we're looking for (trash), and annotations: class labels and bounding boxes.\n",
        "\n",
        "### YOLOv8\n",
        "\n",
        "In this project we will use Ultralytics YOLOv8 object detection.\n",
        "\n",
        "YOLOv8 expects datasets in the following format:\n",
        "\n",
        "```\n",
        "dataset/\n",
        "├── images/\n",
        "│   ├── train/  <-- image files for training\n",
        "│   └── val/    <-- image files for validation after each epoch. Must not overlap with images in train.\n",
        "├── labels/\n",
        "│   ├── train/  <-- one .txt file per train image. Contains class and bbox info..\n",
        "│   └── val/    <-- one .txt file per val image\n",
        "└── data.yaml   <-- config file,\n",
        "```\n",
        "\n",
        "Example labels/train file:\n",
        "```\n",
        "<class_id> <x_center> <y_center> <width> <height>\n",
        "```\n",
        "\n",
        "Example data.yaml file:\n",
        "```\n",
        "path: /content/dataset  # Root folder\n",
        "train: images/train\n",
        "val: images/val\n",
        "\n",
        "nc: 5  # number of classes\n",
        "names: ['bottle', 'can', 'plastic bag', 'wrapper', 'paper']\n",
        "\n",
        "```\n",
        "\n",
        "### Colabs\n",
        "\n",
        "When you open the \"Files\" tab on the left, you'll find yourself in a folder containing\n",
        "* ..\n",
        "* sample data\n",
        "\n",
        "This is a colab thing, the \"content\" folder, to get you started.\n",
        "Ignore it: click the .. to go up a level."
      ],
      "metadata": {
        "id": "x347PAo8rkYn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "⚠️ ***CHANGE THIS FILE FROM HERE DOWN TO SUIT YOUR DATASET*** ⚠️\n",
        "\n",
        "The sections above apply for all dataset conversions.\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "GUP0h7VX_seR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### TACO dataset\n",
        "\n",
        "#### Overview\n",
        "\n",
        "The TACO dataset uses COCO-style formatting (segmentation).\n",
        "\n",
        "Sidebar: COCO, Common Objects in Context, is a object detection, segmentation, and captioning dataset developed by Microsoft. It uses an annotations.json file to organize image data. This json annotation approach has become standard for other datasets to use.\n",
        "\n",
        "So, TACO has an annotations.json file containing:\n",
        "*   \"images\":  List of image metadata\n",
        "*   \"annotations\":  List of label data (type of trash, bounding box definition, segmentation data; corresponds to images list)\n",
        "*   \"categories\":  List of the different categories this dataset uses\n",
        "\n",
        "#### Conversion\n",
        "To convert the TACO dataset to a format YOLOv8 can use, we must:\n",
        "* Split the TACO images into train and val sets\n",
        "* Extract label and bbox info from annotations.json, and save it in individual txt files corresponding to the image files\n",
        "* Make a data.yaml file"
      ],
      "metadata": {
        "id": "f7u6PWyfxeLD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "import shutil\n",
        "import random\n",
        "from pathlib import Path\n",
        "from sklearn.model_selection import train_test_split\n",
        "import yaml\n",
        "\n",
        "# === IMPORT DATASET ===\n",
        "import kagglehub\n",
        "taco_dataset_path = kagglehub.dataset_download('kneroma/tacotrashdataset')  # https://www.kaggle.com/datasets/kneroma/tacotrashdataset\n",
        "print(f\"Dataset downloaded to {taco_dataset_path}\\n\")\n",
        "\n",
        "# === CONFIG ===\n",
        "input_root = Path(taco_dataset_path)\n",
        "output_root = Path('/kaggle/working/taco_yolo')   # Since TACO is a kaggle dataset, this will output to kaggle/working/taco_yolo\n",
        "train_ratio = 0.8  # 80% training, 20% validation\n",
        "\n",
        "# === OUTPUT STRUCTURE ===\n",
        "images_train = output_root / 'images' / 'train'\n",
        "images_val = output_root / 'images' / 'val'\n",
        "labels_train = output_root / 'labels' / 'train'\n",
        "labels_val = output_root / 'labels' / 'val'\n",
        "\n",
        "# Create folders\n",
        "for folder in [images_train, images_val, labels_train, labels_val]:\n",
        "    folder.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "# === LOAD ANNOTATIONS ===\n",
        "with open(input_root / 'data' / 'annotations.json', 'r') as f:\n",
        "    coco = json.load(f)\n",
        "\n",
        "image_id_to_info = {img['id']: img for img in coco['images']}\n",
        "annotations_by_image = {}\n",
        "\n",
        "for ann in coco['annotations']:\n",
        "    image_id = ann['image_id']\n",
        "    annotations_by_image.setdefault(image_id, []).append(ann)\n",
        "\n",
        "category_map = {cat['id']: idx for idx, cat in enumerate(coco['categories'])}\n",
        "category_names = [cat['name'] for cat in sorted(coco['categories'], key=lambda x: category_map[x['id']])]\n",
        "\n",
        "# === SPLIT DATA ===\n",
        "all_image_ids = list(image_id_to_info.keys())\n",
        "train_ids, val_ids = train_test_split(all_image_ids, train_size=train_ratio, random_state=42)\n",
        "\n",
        "def convert_bbox_to_yolo(bbox, img_w, img_h):\n",
        "    x, y, w, h = bbox\n",
        "    x_center = (x + w / 2) / img_w\n",
        "    y_center = (y + h / 2) / img_h\n",
        "    w /= img_w\n",
        "    h /= img_h\n",
        "    return x_center, y_center, w, h\n",
        "\n",
        "def process_image(image_id, split):\n",
        "    image_info = image_id_to_info[image_id]\n",
        "    img_path = input_root / 'data' / image_info['file_name']\n",
        "    img_w, img_h = image_info['width'], image_info['height']\n",
        "\n",
        "    # Output paths\n",
        "    out_img_dir = images_train if split == 'train' else images_val\n",
        "    out_label_dir = labels_train if split == 'train' else labels_val\n",
        "\n",
        "    rel_path = Path(image_info['file_name'])  # e.g., batch_1/000123.jpg\n",
        "    batch_prefix = rel_path.parts[0].replace('/', '_')  # \"batch_1\"\n",
        "    filename = batch_prefix + \"_\" + rel_path.name  # \"batch_1_000123.jpg\"\n",
        "\n",
        "    # Output flattened path\n",
        "    out_img_path = out_img_dir / filename\n",
        "    out_label_path = out_label_dir / filename.replace('.jpg', '.txt')\n",
        "\n",
        "    # Copy image\n",
        "    shutil.copy(input_root / 'data' / rel_path, out_img_path)\n",
        "\n",
        "    # Write label\n",
        "    with open(out_label_path, 'w') as label_file:\n",
        "        for ann in annotations_by_image.get(image_id, []):\n",
        "            class_id = category_map[ann['category_id']]\n",
        "            bbox = convert_bbox_to_yolo(ann['bbox'], img_w, img_h)\n",
        "            label_file.write(f\"{class_id} {' '.join(f'{x:.6f}' for x in bbox)}\\n\")\n",
        "\n",
        "# Process images\n",
        "for image_id in train_ids:\n",
        "    process_image(image_id, 'train')\n",
        "\n",
        "for image_id in val_ids:\n",
        "    process_image(image_id, 'val')\n",
        "\n",
        "# === WRITE data.yaml ===\n",
        "data_yaml = {\n",
        "    'path': str(output_root),\n",
        "    'train': 'images/train',\n",
        "    'val': 'images/val',\n",
        "    'nc': len(category_names),\n",
        "    'names': category_names\n",
        "}\n",
        "\n",
        "with open(output_root / 'data.yaml', 'w') as f:\n",
        "    yaml.dump(data_yaml, f)\n",
        "\n",
        "print(\"Conversion complete. YOLOv8 dataset created at:\", output_root)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aphWONrIrj8y",
        "outputId": "04afc386-5b6c-4d3c-d3e2-7eff6f3d8ebf"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset downloaded to /kaggle/input/tacotrashdataset\n",
            "\n",
            "Conversion complete. YOLOv8 dataset created at: /kaggle/working/taco_yolo\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "To verify that your conversion worked, make sure you can train a model and that it outputs images with a bounding box and label."
      ],
      "metadata": {
        "id": "xpsMcME2-rDs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = YOLO('yolov8n.pt')\n",
        "results = model.train(data='/kaggle/working/taco_yolo/data.yaml', epochs=5, imgsz=640)  # epoch size is small - this is just to see if it can work!"
      ],
      "metadata": {
        "collapsed": true,
        "id": "UL4rIlXU-oLt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "If the model outputs even one image with a bounding box and label, then the dataset should work for our project! Verify this using the code below."
      ],
      "metadata": {
        "id": "2JPNfPNAEtFK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import cv2\n",
        "from random import sample\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "model = YOLO('runs/detect/train/weights/best.pt')\n",
        "\n",
        "train_images_path = Path('/kaggle/working/taco_yolo/images/train')\n",
        "image_files = list(train_images_path.glob('*.jpg'))\n",
        "\n",
        "sample_images = sample(image_files, 10)\n",
        "\n",
        "for image_path in sample_images:\n",
        "    result = model(image_path)[0]\n",
        "    annotated_image = result.plot()\n",
        "\n",
        "    plt.figure(figsize=(8, 6))\n",
        "    plt.imshow(annotated_image)\n",
        "    plt.title(f'Predictions: {image_path.name}')\n",
        "    plt.axis('off')\n",
        "    plt.show()"
      ],
      "metadata": {
        "collapsed": true,
        "id": "Apy8pfX6Eyhe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "If the model successfully generated even one image with a bounding box and label, please download the converted dataset and upload it on Google Drive, https://www.google.com/url?q=https%3A%2F%2Fdrive.google.com%2Fdrive%2Ffolders%2F1bUkIYQRXX08OKI5TuOSg-eqntSudGaFB."
      ],
      "metadata": {
        "id": "zj5c-mtlL2gk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from datetime import datetime\n",
        "\n",
        "dataset_name = 'taco_yolo'\n",
        "\n",
        "# Generate date prefix\n",
        "date_str = datetime.now().strftime('%Y%m%d')\n",
        "zip_name = f\"{date_str}_{dataset_name}.zip\"\n",
        "\n",
        "# Change directory and zip\n",
        "%cd /kaggle/working/{dataset_name}\n",
        "!zip -r /content/{zip_name} .\n",
        "\n",
        "print(f\"✅ Zip created at /content/{zip_name}\")"
      ],
      "metadata": {
        "collapsed": true,
        "id": "Ik8EKrBMLyap"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}