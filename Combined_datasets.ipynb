{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/astrid12345/recyclo/blob/Combine-datasets/Combined_datasets.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n8HefqSf7RbJ",
        "outputId": "17a441e5-17c8-4517-a00b-006d89337908"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "Extracted 20250602z_mju-waste_yolo.zip to /content/20250602z_mju-waste_yolo_extracted\n",
            "Extracted 20250603_TACO_yolo_1500.zip to /content/20250603_TACO_yolo_1500_extracted\n",
            "Extracted 20250610_aquatrash_yolo.zip to /content/20250610_aquatrash_yolo_extracted\n",
            "\n",
            "Found the following zip files:\n",
            "- 20250602z_mju-waste_yolo.zip\n",
            "- 20250603_TACO_yolo_1500.zip\n",
            "- 20250610_aquatrash_yolo.zip\n",
            "\n",
            "Extracted dataset names:\n",
            "- /content/20250602z_mju-waste_yolo_extracted\n",
            "- /content/20250603_TACO_yolo_1500_extracted\n",
            "- /content/20250610_aquatrash_yolo_extracted\n"
          ]
        }
      ],
      "source": [
        "#Reading 3 zipped datasets from Google Drive and unzipping them\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "import zipfile\n",
        "import os\n",
        "\n",
        "# Specify the path to the folder in your Google Drive after adding a shortcut\n",
        "drive_folder_path = '/content/drive/MyDrive/Recyclo/datasets'\n",
        "\n",
        "# List to store the names of found zip files\n",
        "found_zip_files = []\n",
        "\n",
        "# Initialize the list to store extracted dataset names before the loop\n",
        "extracted_dataset_names = []\n",
        "\n",
        "# Check if the directory exists before listing its contents\n",
        "if not os.path.exists(drive_folder_path):\n",
        "    print(f\"Error: The directory '{drive_folder_path}' was not found.\")\n",
        "    print(\"Please ensure you have added a shortcut to the shared folder in your 'My Drive' and the path is correct.\")\n",
        "else:\n",
        "    # List all files in the folder\n",
        "    for filename in os.listdir(drive_folder_path):\n",
        "      if filename.endswith(\".zip\"):\n",
        "        # Add the found zip file name to the list\n",
        "        found_zip_files.append(filename)\n",
        "\n",
        "        zip_path = os.path.join(drive_folder_path, filename)\n",
        "\n",
        "        # Create a directory to extract to (optional)\n",
        "        # Ensure the filename is clean for directory naming\n",
        "        safe_filename = filename.replace('.zip', '')\n",
        "        extract_path = f'/content/{safe_filename}_extracted'\n",
        "        # Append the extracted path to the list within the loop\n",
        "        extracted_dataset_names.append(extract_path)\n",
        "\n",
        "        os.makedirs(extract_path, exist_ok=True)\n",
        "\n",
        "        # Open the zip file\n",
        "        try:\n",
        "            with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
        "              # Extract all the contents into the specified directory\n",
        "              zip_ref.extractall(extract_path)\n",
        "              print(f\"Extracted {filename} to {extract_path}\")\n",
        "        except zipfile.BadZipFile:\n",
        "            print(f\"Error: {filename} is not a valid zip file.\")\n",
        "        except FileNotFoundError:\n",
        "             print(f\"Error: Zip file not found at {zip_path}. This is unexpected after listing the directory.\")\n",
        "\n",
        "# Optional: Print the list of found zip files after the loop\n",
        "print(\"\\nFound the following zip files:\")\n",
        "for zip_name in found_zip_files:\n",
        "    print(f\"- {zip_name}\")\n",
        "\n",
        "print(\"\\nExtracted dataset names:\")\n",
        "for name in extracted_dataset_names:\n",
        "  print(f\"- {name}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BWhAwxS3dz3I",
        "outputId": "5081ce43-9dba-405b-ed49-7f7c20f6bd7d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Deleted directory and its contents: /content/20250602z_mju-waste_yolo_extracted\n",
            "Deleted directory and its contents: /content/20250610_aquatrash_yolo_extracted\n",
            "Deleted directory and its contents: /content/20250603_TACO_yolo_1500_extracted\n",
            "Deleted directory and its contents: /content/merged_yolo_dataset\n"
          ]
        }
      ],
      "source": [
        "\"\"\"import shutil\n",
        "import os\n",
        "\n",
        "# List of directories to delete\n",
        "directories_to_delete = [\n",
        "    '/content/20250602z_mju-waste_yolo_extracted',\n",
        "    '/content/20250610_aquatrash_yolo_extracted',\n",
        "    '/content/20250603_TACO_yolo_1500_extracted',\n",
        "    '/content/merged_yolo_dataset'\n",
        "] # Replace with your actual list of folder paths\n",
        "\n",
        "for directory_path in directories_to_delete:\n",
        "    if os.path.exists(directory_path):\n",
        "        try:\n",
        "            shutil.rmtree(directory_path)\n",
        "            print(f\"Deleted directory and its contents: {directory_path}\")\n",
        "        except OSError as e:\n",
        "            print(f\"Error deleting directory {directory_path}: {e}\")\n",
        "    else:\n",
        "        print(f\"Directory not found: {directory_path}\")\"\"\""
      ]
    },
    {
      "source": [
        "\"\"\"\n",
        "- Merges 3 YOLO datasets while preserving the original train, val, and test splits.\n",
        "- Maps all labels to a single one, \"trash\".\n",
        "- Keeps dataset name prefixes in all image and label names.\n",
        "- Creates a .yaml file for the merged dataset.\n",
        "- Skips images with no label files.\n",
        "\"\"\"\n",
        "\n",
        "# %%\n",
        "import os\n",
        "import shutil\n",
        "import yaml\n",
        "# No need for 'random' if not reshuffling\n",
        "# import random\n",
        "\n",
        "# --- Configuration ---\n",
        "# List of paths to the extracted YOLO datasets.\n",
        "# This list is now populated correctly from the previous code block.\n",
        "\n",
        "extracted_yolo_dataset_paths = [\n",
        "    '/content/20250602z_mju-waste_yolo_extracted',\n",
        "    '/content/20250603_TACO_yolo_1500_extracted'\n",
        "    ]\n",
        "\n",
        "aquatrash_extracted_base_path = '/content/20250610_aquatrash_yolo_extracted'\n",
        "\n",
        "# Path to the AquaTrash_yolo directory within the extraction\n",
        "aquatrash_yolo_path = os.path.join(aquatrash_extracted_base_path, 'kaggle', 'working', 'AquaTrash_yolo')\n",
        "\n",
        "# Output directory for the merged YOLO dataset\n",
        "merged_output_dir = '/content/merged_yolo_dataset' # Changed back to the original name\n",
        "\n",
        "# Define a mapping for merging class names\n",
        "# Key: original class name from a dataset\n",
        "# Value: the desired class name in the merged dataset\n",
        "class_name_mapping = {\n",
        "    'Rubbish': 'trash',\n",
        "    'trash': 'trash',\n",
        "    'glass': 'trash',\n",
        "    'metal': 'trash',\n",
        "    'paper': 'trash',\n",
        "    'plastic': 'trash'\n",
        "}\n",
        "\n",
        "# --- Merging Process ---\n",
        "\n",
        "# Create the main merged directories and split subdirectories\n",
        "merged_images_base = os.path.join(merged_output_dir, 'images')\n",
        "merged_labels_base = os.path.join(merged_output_dir, 'labels')\n",
        "\n",
        "# Create train, val, test subdirectories within the merged structure\n",
        "for subdir in ['train', 'val', 'test']:\n",
        "    os.makedirs(os.path.join(merged_images_base, subdir), exist_ok=True)\n",
        "    os.makedirs(os.path.join(merged_labels_base, subdir), exist_ok=True)\n",
        "\n",
        "# Keep track of encountered class names and map them to new indices\n",
        "class_names_set = set()\n",
        "combined_class_names = []\n",
        "class_name_to_id = {}\n",
        "\n",
        "datasets_to_process = extracted_yolo_dataset_paths + [aquatrash_yolo_path]\n",
        "\n",
        "# --- Step 1: Collect and Map Class Names from all datasets ---\n",
        "# This step remains the same as we need the final class map before processing labels.\n",
        "print(\"Collecting and mapping class names from all data.yaml files...\")\n",
        "\n",
        "for dataset_path in datasets_to_process:\n",
        "    print(f\"Processing data.yaml for: {dataset_path}\")\n",
        "    data_yaml_path = os.path.join(dataset_path, 'data.yaml')\n",
        "\n",
        "    if os.path.exists(data_yaml_path):\n",
        "        try:\n",
        "            with open(data_yaml_path, 'r') as f:\n",
        "                current_data_yaml = yaml.safe_load(f)\n",
        "                current_class_names = current_data_yaml.get('names', [])\n",
        "                print(f\"  Found original classes in {os.path.basename(dataset_path)}: {current_class_names}\")\n",
        "\n",
        "            # Update combined class names and IDs based on mapping\n",
        "            for original_class_name in current_class_names:\n",
        "                mapped_class_name = class_name_mapping.get(original_class_name, original_class_name)\n",
        "                if mapped_class_name not in class_names_set:\n",
        "                    class_names_set.add(mapped_class_name)\n",
        "                    combined_class_names.append(mapped_class_name)\n",
        "\n",
        "            print(f\"  Combined unique mapped classes so far: {combined_class_names}\")\n",
        "\n",
        "        except FileNotFoundError:\n",
        "             print(f\"  Warning: data.yaml not found at {data_yaml_path}. Cannot get class names from this dataset.\")\n",
        "        except Exception as e:\n",
        "             print(f\"  Error loading data.yaml from {data_yaml_path}: {e}. Skipping class processing for this dataset.\")\n",
        "    else:\n",
        "         print(f\"  Warning: data.yaml not found at {data_yaml_path}. Cannot get class names from this dataset.\")\n",
        "\n",
        "# After processing all data.yaml files, sort combined_class_names to have consistent ID assignment\n",
        "combined_class_names.sort()\n",
        "class_name_to_id = {name: i for i, name in enumerate(combined_class_names)}\n",
        "print(\"\\nFinal Combined Class Names and IDs:\")\n",
        "print(combined_class_names)\n",
        "print(class_name_to_id)\n",
        "print(f\"Total combined classes: {len(combined_class_names)}\")\n",
        "\n",
        "\n",
        "# --- Step 2: Copy and Process Files while preserving splits ---\n",
        "print(\"\\nCopying and processing files to merged dataset while preserving splits...\")\n",
        "\n",
        "for dataset_path in datasets_to_process:\n",
        "    print(f\"Processing dataset: {dataset_path} for file copying\")\n",
        "\n",
        "    # Extract a simple prefix from the dataset path\n",
        "    # Handle the AquaTrash path specifically if needed, or use a consistent approach\n",
        "    # Ensure the prefix is safe for filenames\n",
        "    if 'AquaTrash_yolo' in dataset_path:\n",
        "        dataset_prefix = 'aquatrash_yolo_'\n",
        "    elif 'mju-waste_yolo' in dataset_path:\n",
        "         dataset_prefix = 'mju_waste_yolo_'\n",
        "    elif 'TACO_yolo_1500' in dataset_path:\n",
        "         dataset_prefix = 'taco_yolo_1500_'\n",
        "    else:\n",
        "        # Fallback prefix if the path doesn't match expected patterns\n",
        "        dataset_prefix = os.path.basename(dataset_path).replace('_extracted', '').replace('-', '_').lower() + '_'\n",
        "\n",
        "    # Load class names for the current dataset again for annotation processing\n",
        "    data_yaml_path = os.path.join(dataset_path, 'data.yaml')\n",
        "    current_class_id_to_name = {}\n",
        "    if os.path.exists(data_yaml_path):\n",
        "        try:\n",
        "            with open(data_yaml_path, 'r') as f:\n",
        "                current_data_yaml = yaml.safe_load(f)\n",
        "                current_class_names = current_data_yaml.get('names', [])\n",
        "                current_class_id_to_name = {i: name for i, name in enumerate(current_class_names)}\n",
        "        except Exception as e:\n",
        "             print(f\"  Warning: Error loading data.yaml again from {data_yaml_path} for file copying: {e}\")\n",
        "             current_class_id_to_name = {}\n",
        "    else:\n",
        "        print(f\"  Warning: data.yaml not found at {data_yaml_path} during file copying.\")\n",
        "        current_class_id_to_name = {} # Cannot map class IDs if data.yaml is missing\n",
        "\n",
        "    # Iterate through original splits and copy to corresponding merged splits\n",
        "    for split_subdir in ['train', 'val', 'test']:\n",
        "        original_images_path = os.path.join(dataset_path, 'images', split_subdir)\n",
        "        original_labels_path = os.path.join(dataset_path, 'labels', split_subdir)\n",
        "        merged_split_images_path = os.path.join(merged_images_base, split_subdir)\n",
        "        merged_split_labels_path = os.path.join(merged_labels_base, split_subdir)\n",
        "\n",
        "        if not os.path.exists(original_images_path):\n",
        "            print(f\"  Info: Image directory not found for {split_subdir} in {os.path.basename(dataset_path)}: {original_images_path}. Skipping this split.\")\n",
        "            continue\n",
        "        if not os.path.exists(original_labels_path):\n",
        "            print(f\"  Info: Label directory not found for {split_subdir} in {os.path.basename(dataset_path)}: {original_labels_path}. Skipping this split.\")\n",
        "            continue\n",
        "\n",
        "        print(f\"  Copying {split_subdir} images and labels from {os.path.basename(dataset_path)}...\")\n",
        "\n",
        "        for filename in os.listdir(original_images_path):\n",
        "            img_name, img_ext = os.path.splitext(filename)\n",
        "            label_filename = img_name + '.txt'\n",
        "\n",
        "            original_image_file = os.path.join(original_images_path, filename)\n",
        "            original_label_file = os.path.join(original_labels_path, label_filename)\n",
        "\n",
        "            # Check if corresponding label file exists\n",
        "            if not os.path.exists(original_label_file):\n",
        "                print(f\"    Warning: Label file not found for image {filename} at {original_labels_path}. Skipping this image and its missing label.\")\n",
        "                continue\n",
        "\n",
        "            # Construct new filenames with dataset prefix\n",
        "            new_img_filename = f\"{dataset_prefix}{filename}\"\n",
        "            new_label_filename = f\"{dataset_prefix}{label_filename}\"\n",
        "\n",
        "            merged_dest_image_path = os.path.join(merged_split_images_path, new_img_filename)\n",
        "            merged_dest_label_path = os.path.join(merged_split_labels_path, new_label_filename)\n",
        "\n",
        "            try:\n",
        "                # Copy files\n",
        "                shutil.copy2(original_image_file, merged_dest_image_path)\n",
        "\n",
        "                # --- Update Class Indices in Label File ---\n",
        "                updated_lines = []\n",
        "                # Open the original label file for reading and the new label file for writing\n",
        "                with open(original_label_file, 'r') as infile, open(merged_dest_label_path, 'w') as outfile:\n",
        "                    for line in infile:\n",
        "                        parts = line.strip().split()\n",
        "                        if parts and len(parts) == 5:\n",
        "                            try:\n",
        "                                original_class_id = int(parts[0])\n",
        "                                # Use the class map specific to the original dataset this file came from\n",
        "\n",
        "                                # --- Added check for key existence ---\n",
        "                                if current_class_id_to_name and original_class_id in current_class_id_to_name:\n",
        "                                    original_class_name = current_class_id_to_name[original_class_id]\n",
        "                                else:\n",
        "                                     # Handle case where original class ID is not found in the map\n",
        "                                     print(f\"    Warning: Original class ID {original_class_id} not found in original class map for {new_label_filename}. Skipping annotation line: '{line.strip()}'\")\n",
        "                                     continue # Skip this line\n",
        "\n",
        "                                mapped_class_name = class_name_mapping.get(original_class_name, original_class_name)\n",
        "\n",
        "                                # Check if the mapped class name is in the final combined class map\n",
        "                                if mapped_class_name in class_name_to_id:\n",
        "                                    new_class_id = class_name_to_id[mapped_class_name]\n",
        "                                    parts[0] = str(new_class_id)\n",
        "                                    updated_lines.append(\" \".join(parts))\n",
        "                                else:\n",
        "                                     # This warning indicates an issue with class mapping or data.yaml loading\n",
        "                                     print(f\"    Warning: Mapped class name '{mapped_class_name}' (original: '{original_class_name}') not found in combined class map for {new_label_filename}. Skipping annotation line: '{line.strip()}'\")\n",
        "                                     pass\n",
        "                            except ValueError:\n",
        "                                print(f\"    Warning: Invalid class ID format in label file {new_label_filename} line: '{line.strip()}'. Skipping this line.\")\n",
        "                                pass # Skip lines with invalid format\n",
        "                            except Exception as line_e:\n",
        "                                print(f\"    Error processing line '{line.strip()}' in {new_label_filename}: {line_e}. Skipping this line.\")\n",
        "                                pass # Skip lines with other errors\n",
        "                        else:\n",
        "                             print(f\"    Warning: Skipping malformed line in {new_label_filename}: '{line.strip()}'\")\n",
        "\n",
        "                    # Write the processed lines to the new label file\n",
        "                    outfile.write(\"\\n\".join(updated_lines))\n",
        "\n",
        "            except FileNotFoundError:\n",
        "                 print(f\"    Error copying files for {filename}. Image or label file not found unexpectedly.\")\n",
        "            except Exception as e:\n",
        "                 print(f\"    Error processing file {filename}: {e}\")\n",
        "\n",
        "print(\"\\nYOLO dataset merging complete.\")\n",
        "\n",
        "# --- Create Merged data.yaml ---\n",
        "print(\"Creating merged data.yaml...\")\n",
        "\n",
        "merged_data_yaml = {\n",
        "    'path': merged_output_dir,\n",
        "    'train': os.path.join('images', 'train'), # Relative path to the train images directory\n",
        "    'val': os.path.join('images', 'val'),     # Relative path to the val images directory\n",
        "    'test': os.path.join('images', 'test'),   # Relative path to the test images directory\n",
        "    'nc': len(combined_class_names),\n",
        "    'names': combined_class_names\n",
        "}\n",
        "\n",
        "# Save the merged data.yaml file\n",
        "merged_data_yaml_path = os.path.join(merged_output_dir, 'data.yaml')\n",
        "with open(merged_data_yaml_path, 'w') as f:\n",
        "    yaml.dump(merged_data_yaml, f, sort_keys=False) # Use sort_keys=False to keep names order consistent\n",
        "\n",
        "print(f\"Merged data.yaml saved to: {merged_data_yaml_path}\")\n",
        "print(f\"Total classes in merged dataset: {len(combined_class_names)}\")\n",
        "print(f\"Combined class names: {combined_class_names}\")\n",
        "\n",
        "# --- Next Steps ---\n",
        "print(\"\\nNext Steps:\")\n",
        "print(f\"1. Your merged YOLO dataset is located at: {merged_output_dir}\")\n",
        "print(f\"2. The data.yaml file is in: {merged_data_yaml_path}\")\n",
        "print(\"3. This dataset contains images and labels from the original train, val, and test splits, merged into corresponding subdirectories, with original dataset prefixes.\")\n",
        "print(\"4. You can now use this merged dataset for training with YOLO.\")"
      ],
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7oJsY3gh5Zsk",
        "outputId": "e99aba5f-abe8-43b7-9012-170f3f42b284"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting and mapping class names from all data.yaml files...\n",
            "Processing data.yaml for: /content/20250602z_mju-waste_yolo_extracted\n",
            "  Found original classes in 20250602z_mju-waste_yolo_extracted: ['Rubbish']\n",
            "  Combined unique mapped classes so far: ['trash']\n",
            "Processing data.yaml for: /content/20250603_TACO_yolo_1500_extracted\n",
            "  Found original classes in 20250603_TACO_yolo_1500_extracted: ['trash']\n",
            "  Combined unique mapped classes so far: ['trash']\n",
            "Processing data.yaml for: /content/20250610_aquatrash_yolo_extracted/kaggle/working/AquaTrash_yolo\n",
            "  Found original classes in AquaTrash_yolo: ['trash']\n",
            "  Combined unique mapped classes so far: ['trash']\n",
            "\n",
            "Final Combined Class Names and IDs:\n",
            "['trash']\n",
            "{'trash': 0}\n",
            "Total combined classes: 1\n",
            "\n",
            "Copying and processing files to merged dataset while preserving splits...\n",
            "Processing dataset: /content/20250602z_mju-waste_yolo_extracted for file copying\n",
            "  Copying train images and labels from 20250602z_mju-waste_yolo_extracted...\n",
            "    Warning: Label file not found for image 2019-12-10_16_14_32-04_color.png at /content/20250602z_mju-waste_yolo_extracted/labels/train. Skipping this image and its missing label.\n",
            "    Warning: Label file not found for image 2019-12-04_16_24_27-41_color.png at /content/20250602z_mju-waste_yolo_extracted/labels/train. Skipping this image and its missing label.\n",
            "    Warning: Label file not found for image 2019-09-19_16_20_30-29_color.png at /content/20250602z_mju-waste_yolo_extracted/labels/train. Skipping this image and its missing label.\n",
            "    Warning: Label file not found for image 2019-12-10_16_34_16-66_color.png at /content/20250602z_mju-waste_yolo_extracted/labels/train. Skipping this image and its missing label.\n",
            "    Warning: Label file not found for image 2019-09-19_16_20_37-17_color.png at /content/20250602z_mju-waste_yolo_extracted/labels/train. Skipping this image and its missing label.\n",
            "    Warning: Label file not found for image 2019-12-04_16_17_40-33_color.png at /content/20250602z_mju-waste_yolo_extracted/labels/train. Skipping this image and its missing label.\n",
            "    Warning: Label file not found for image 2019-12-04_16_20_08-09_color.png at /content/20250602z_mju-waste_yolo_extracted/labels/train. Skipping this image and its missing label.\n",
            "    Warning: Label file not found for image 2019-09-19_16_37_35-52_color.png at /content/20250602z_mju-waste_yolo_extracted/labels/train. Skipping this image and its missing label.\n",
            "  Copying val images and labels from 20250602z_mju-waste_yolo_extracted...\n",
            "  Copying test images and labels from 20250602z_mju-waste_yolo_extracted...\n",
            "Processing dataset: /content/20250603_TACO_yolo_1500_extracted for file copying\n",
            "  Copying train images and labels from 20250603_TACO_yolo_1500_extracted...\n",
            "  Copying val images and labels from 20250603_TACO_yolo_1500_extracted...\n",
            "  Copying test images and labels from 20250603_TACO_yolo_1500_extracted...\n",
            "Processing dataset: /content/20250610_aquatrash_yolo_extracted/kaggle/working/AquaTrash_yolo for file copying\n",
            "  Copying train images and labels from AquaTrash_yolo...\n",
            "  Copying val images and labels from AquaTrash_yolo...\n",
            "  Copying test images and labels from AquaTrash_yolo...\n",
            "\n",
            "YOLO dataset merging complete.\n",
            "Creating merged data.yaml...\n",
            "Merged data.yaml saved to: /content/merged_yolo_dataset/data.yaml\n",
            "Total classes in merged dataset: 1\n",
            "Combined class names: ['trash']\n",
            "\n",
            "Next Steps:\n",
            "1. Your merged YOLO dataset is located at: /content/merged_yolo_dataset\n",
            "2. The data.yaml file is in: /content/merged_yolo_dataset/data.yaml\n",
            "3. This dataset contains images and labels from the original train, val, and test splits, merged into corresponding subdirectories, with original dataset prefixes.\n",
            "4. You can now use this merged dataset for training with YOLO.\n"
          ]
        }
      ]
    },
    {
      "source": [
        "# Summarizing the % of images from all 3 datasets in train, val, test datasets\n",
        "# %%\n",
        "import os\n",
        "\n",
        "# Path to your merged dataset directory\n",
        "merged_output_dir = '/content/merged_yolo_dataset' # Ensure this matches your output directory\n",
        "\n",
        "# Define the expected dataset prefixes you used during merging\n",
        "# These should match the ones generated from your original zip file names\n",
        "dataset_prefixes = [\n",
        "    'mju_waste_yolo_', # Derived from '20250602z_mju-waste_yolo.zip'\n",
        "    'aquatrash_yolo_', # Derived from '20250610_aquatrash_yolo.zip' (corrected based on your path logic)\n",
        "    'taco_yolo_1500_'  # Derived from '20250603_TACO_yolo_1500.zip'\n",
        "]\n",
        "\n",
        "# Dictionary to store counts\n",
        "# Structure: {split: {prefix: count}}\n",
        "image_counts_by_dataset = {}\n",
        "\n",
        "# Dictionary to store total counts per split\n",
        "total_images_per_split = {}\n",
        "\n",
        "print(\"\\nCounting images from each original dataset in train, val, and test splits...\")\n",
        "\n",
        "total_image_label_pairs_collected = 0 # Initialize a counter for total images\n",
        "\n",
        "# Iterate through each split (train, val, test)\n",
        "for split_subdir in ['train', 'val', 'test']:\n",
        "    split_images_path = os.path.join(merged_output_dir, 'images', split_subdir)\n",
        "    image_counts_by_dataset[split_subdir] = {prefix: 0 for prefix in dataset_prefixes} # Initialize counts for this split\n",
        "    total_images_per_split[split_subdir] = 0 # Initialize total for this split\n",
        "\n",
        "    if os.path.exists(split_images_path):\n",
        "        print(f\"Processing split: {split_subdir}\")\n",
        "        # List all files in the image directory for the current split\n",
        "        for filename in os.listdir(split_images_path):\n",
        "            # Check if the file is an image (you might want to add more robust checks for image extensions)\n",
        "            if filename.lower().endswith(('.png', '.jpg', '.jpeg', '.bmp', '.tiff')):\n",
        "                # Try to match the filename with a known dataset prefix\n",
        "                matched_prefix = None\n",
        "                for prefix in dataset_prefixes:\n",
        "                    if filename.startswith(prefix):\n",
        "                        matched_prefix = prefix\n",
        "                        break # Found a match, no need to check other prefixes\n",
        "\n",
        "                if matched_prefix:\n",
        "                    # Increment the count for the matched prefix in the current split\n",
        "                    image_counts_by_dataset[split_subdir][matched_prefix] += 1\n",
        "                    total_images_per_split[split_subdir] += 1 # Increment total for this split\n",
        "                    total_image_label_pairs_collected += 1 # Increment overall total\n",
        "                else:\n",
        "                    # Handle files that don't match any expected prefix (shouldn't happen if merging worked as expected)\n",
        "                    print(f\"  Warning: Image '{filename}' in {split_subdir} does not match any known dataset prefix.\")\n",
        "    else:\n",
        "        print(f\"  Info: Image directory for {split_subdir} not found at {split_images_path}\")\n",
        "\n",
        "# Print the total number of image-label pairs collected\n",
        "print(f\"\\nTotal image-label pairs collected across all merged splits: {total_image_label_pairs_collected}\")\n",
        "\n",
        "\n",
        "# Print the results per dataset per split\n",
        "print(\"\\n--- Image Counts Summary (per dataset within each split) ---\")\n",
        "for split_name, prefix_counts in image_counts_by_dataset.items():\n",
        "    print(f\"\\nSplit: {split_name}\")\n",
        "    total_in_split = sum(prefix_counts.values()) # This should match total_images_per_split[split_name]\n",
        "    if total_in_split > 0:\n",
        "        for prefix, count in prefix_counts.items():\n",
        "            # Clean up prefix for display\n",
        "            display_name = prefix.strip('_').replace('_yolo', '').replace('_', ' ')\n",
        "            print(f\"  - {display_name}: {count} images ({count/total_in_split:.1%})\")\n",
        "        print(f\"  Total images in {split_name} split: {total_in_split}\")\n",
        "    else:\n",
        "        print(f\"  No images found in the {split_name} split.\")\n",
        "\n",
        "# Print the overall train/val/test split percentages\n",
        "print(\"\\n--- Overall Train/Val/Test Split Summary ---\")\n",
        "\n",
        "if total_image_label_pairs_collected > 0:\n",
        "    for split_name, count in total_images_per_split.items():\n",
        "        percentage = (count / total_image_label_pairs_collected) * 100 if total_image_label_pairs_collected > 0 else 0\n",
        "        print(f\"  - {split_name}: {count} images ({percentage:.1f}%)\")\n",
        "else:\n",
        "    print(\"No images found in the merged dataset to calculate split percentages.\")\n",
        "\n",
        "\n",
        "print(\"\\n--- End of Summary ---\")"
      ],
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zXs_DznQsfhw",
        "outputId": "a341cc2c-0d68-406e-d8bb-5b4b3c9f9872"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Counting images from each original dataset in train, val, and test splits...\n",
            "Processing split: train\n",
            "Processing split: val\n",
            "Processing split: test\n",
            "\n",
            "Total image-label pairs collected across all merged splits: 4336\n",
            "\n",
            "--- Image Counts Summary (per dataset within each split) ---\n",
            "\n",
            "Split: train\n",
            "  - mju waste: 1477 images (49.7%)\n",
            "  - aquatrash: 295 images (9.9%)\n",
            "  - taco 1500: 1200 images (40.4%)\n",
            "  Total images in train split: 2972\n",
            "\n",
            "Split: val\n",
            "  - mju waste: 248 images (57.0%)\n",
            "  - aquatrash: 37 images (8.5%)\n",
            "  - taco 1500: 150 images (34.5%)\n",
            "  Total images in val split: 435\n",
            "\n",
            "Split: test\n",
            "  - mju waste: 742 images (79.9%)\n",
            "  - aquatrash: 37 images (4.0%)\n",
            "  - taco 1500: 150 images (16.1%)\n",
            "  Total images in test split: 929\n",
            "\n",
            "--- Overall Train/Val/Test Split Summary ---\n",
            "  - train: 2972 images (68.5%)\n",
            "  - val: 435 images (10.0%)\n",
            "  - test: 929 images (21.4%)\n",
            "\n",
            "--- End of Summary ---\n"
          ]
        }
      ]
    },
    {
      "source": [
        "# Summarizing the total number of pairs in each of 3 datasets\n",
        "# %%\n",
        "import os\n",
        "\n",
        "# List of paths to the original extracted YOLO datasets.\n",
        "# Ensure these paths match the ones used earlier in your notebook.\n",
        "original_dataset_paths = [\n",
        "    '/content/20250602z_mju-waste_yolo_extracted',\n",
        "    '/content/20250603_TACO_yolo_1500_extracted',\n",
        "    '/content/20250610_aquatrash_yolo_extracted/kaggle/working/AquaTrash_yolo' # Explicit path for AquaTrash\n",
        "]\n",
        "\n",
        "# Dictionary to store counts from original datasets\n",
        "# Structure: {dataset_name: total_pairs}\n",
        "original_dataset_counts = {}\n",
        "\n",
        "print(\"Counting image-label pairs in original datasets...\")\n",
        "\n",
        "for dataset_path in original_dataset_paths:\n",
        "    dataset_name = os.path.basename(dataset_path)\n",
        "    total_pairs_in_dataset = 0\n",
        "    print(f\"Processing original dataset: {dataset_name}\")\n",
        "\n",
        "    # Iterate through potential splits in the original dataset\n",
        "    for split_subdir in ['train', 'val', 'test']:\n",
        "        original_images_path = os.path.join(dataset_path, 'images', split_subdir)\n",
        "        original_labels_path = os.path.join(dataset_path, 'labels', split_subdir)\n",
        "\n",
        "        if os.path.exists(original_images_path) and os.path.exists(original_labels_path):\n",
        "            # List files in the image directory for the current split\n",
        "            for filename in os.listdir(original_images_path):\n",
        "                 # Check if the file is an image and has a corresponding label\n",
        "                img_name, img_ext = os.path.splitext(filename)\n",
        "                label_filename = img_name + '.txt'\n",
        "                original_label_file = os.path.join(original_labels_path, label_filename)\n",
        "\n",
        "                if filename.lower().endswith(('.png', '.jpg', '.jpeg', '.bmp', '.tiff')) and os.path.exists(original_label_file):\n",
        "                    total_pairs_in_dataset += 1\n",
        "        # else:\n",
        "             # Optional: Uncomment if you want info about missing splits in original datasets\n",
        "             # print(f\"  Info: Split '{split_subdir}' not found in {dataset_name}\")\n",
        "\n",
        "    original_dataset_counts[dataset_name] = total_pairs_in_dataset\n",
        "    print(f\"  Found {total_pairs_in_dataset} image-label pairs in {dataset_name}\")\n"
      ],
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D_vHlBkhti4c",
        "outputId": "332fa853-e024-40c8-c682-d3adc93cef73"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Counting image-label pairs in original datasets...\n",
            "Processing original dataset: 20250602z_mju-waste_yolo_extracted\n",
            "  Found 2467 image-label pairs in 20250602z_mju-waste_yolo_extracted\n",
            "Processing original dataset: 20250603_TACO_yolo_1500_extracted\n",
            "  Found 1500 image-label pairs in 20250603_TACO_yolo_1500_extracted\n",
            "Processing original dataset: AquaTrash_yolo\n",
            "  Found 369 image-label pairs in AquaTrash_yolo\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOayz+8seYZDCsG3FoM73tn",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}